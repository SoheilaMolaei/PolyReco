{"cells":[{"cell_type":"code","execution_count":177,"metadata":{"id":"cZmK1gXe2zb_","outputId":"ece1d2df-ceb4-46fe-ed9f-fc58472f9a2a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741525458346,"user_tz":0,"elapsed":30891,"user":{"displayName":"Soheila Molaei","userId":"07707104183566116527"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["- Total BCP data: 112\n","- Total Homopolymer data: 404\n","Epoch 0, Loss: 3163.3906, Val Loss: 4351.3379\n","Epoch 20, Loss: 1.5316, Val Loss: 1.3081\n","Epoch 40, Loss: 1.4146, Val Loss: 1.3524\n","Epoch 60, Loss: 1.3860, Val Loss: 1.3748\n","Epoch 80, Loss: 1.3863, Val Loss: 1.3936\n","Epoch 100, Loss: 1.3860, Val Loss: 1.3954\n","Epoch 120, Loss: 1.3866, Val Loss: 1.3959\n","Epoch 140, Loss: 1.3861, Val Loss: 1.3963\n","Epoch 160, Loss: 1.3852, Val Loss: 1.3965\n","Epoch 180, Loss: 1.3859, Val Loss: 1.3968\n","Epoch 200, Loss: 1.3857, Val Loss: 1.3970\n","Epoch 220, Loss: 1.3859, Val Loss: 1.3969\n","Epoch 240, Loss: 1.3860, Val Loss: 1.3970\n","Epoch 260, Loss: 1.3859, Val Loss: 1.3969\n","Epoch 280, Loss: 1.3866, Val Loss: 1.3968\n","Epoch 300, Loss: 1.3856, Val Loss: 1.3966\n","Epoch 320, Loss: 1.3868, Val Loss: 1.3966\n","Epoch 340, Loss: 1.3869, Val Loss: 1.3966\n","Epoch 360, Loss: 1.3858, Val Loss: 1.3966\n","Epoch 380, Loss: 1.3861, Val Loss: 1.3966\n","Epoch 400, Loss: 1.3862, Val Loss: 1.3966\n","Epoch 420, Loss: 1.3863, Val Loss: 1.3967\n","Epoch 440, Loss: 1.3861, Val Loss: 1.3965\n","Epoch 460, Loss: 1.3858, Val Loss: 1.3964\n","Epoch 480, Loss: 1.3859, Val Loss: 1.3966\n","Fold 1, AUC: 0.8945\n","Epoch 0, Loss: 3497.1082, Val Loss: 2454.9536\n","Epoch 20, Loss: 1.7098, Val Loss: 10.8177\n","Epoch 40, Loss: 34.9796, Val Loss: 1.1890\n","Epoch 60, Loss: 1.2538, Val Loss: 1.1547\n","Epoch 80, Loss: 1.3125, Val Loss: 1.1482\n","Epoch 100, Loss: 1.2886, Val Loss: 1.1454\n","Epoch 120, Loss: 1.2635, Val Loss: 1.1448\n","Epoch 140, Loss: 1.3141, Val Loss: 1.1447\n","Epoch 160, Loss: 1.2877, Val Loss: 1.1446\n","Epoch 180, Loss: 1.3369, Val Loss: 1.1437\n","Epoch 200, Loss: 1.3655, Val Loss: 1.1434\n","Epoch 220, Loss: 1.2954, Val Loss: 1.1436\n","Epoch 240, Loss: 1.2305, Val Loss: 1.1435\n","Epoch 260, Loss: 1.2876, Val Loss: 1.1433\n","Epoch 280, Loss: 1.2673, Val Loss: 1.1432\n","Epoch 300, Loss: 1.3594, Val Loss: 1.1431\n","Epoch 320, Loss: 1.2905, Val Loss: 1.1430\n","Epoch 340, Loss: 1.2406, Val Loss: 1.1429\n","Epoch 360, Loss: 1.3123, Val Loss: 1.1428\n","Epoch 380, Loss: 1.2472, Val Loss: 1.1427\n","Epoch 400, Loss: 1.2859, Val Loss: 1.1424\n","Epoch 420, Loss: 1.2680, Val Loss: 1.1421\n","Epoch 440, Loss: 1.3093, Val Loss: 1.1421\n","Epoch 460, Loss: 1.2655, Val Loss: 1.1421\n","Epoch 480, Loss: 1.2630, Val Loss: 1.1419\n","Fold 2, AUC: 1.0000\n"]}],"source":["import math\n","import copy\n","import itertools\n","import random\n","import numpy as np\n","import pandas as pd\n","import networkx as nx\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import dgl\n","import dgl.function as fn\n","from rdkit import Chem\n","from rdkit.Chem.Crippen import MolLogP, MolMR\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import train_test_split, KFold\n","\n","# =============================================================================\n","# Helper Functions\n","# =============================================================================\n","\n","def get_hard_soft_value(smile, dataframe):\n","    \"\"\"\n","    Get the 'Hard/Soft' value from the dataframe based on the smile string.\n","    \"\"\"\n","    filtered_df = dataframe[dataframe['Big_Smile'] == smile]\n","    return filtered_df.iloc[0]['Hard/Soft'] if not filtered_df.empty else None\n","\n","def graph_features(mol, atom_labels, max_length=None):\n","    \"\"\"\n","    Compute one‐hot features for the atoms in a molecule.\n","    Pads the feature matrix up to max_length.\n","    \"\"\"\n","    max_length = max_length or mol.GetNumAtoms()\n","    features = np.array([[a.GetAtomicNum() == i for i in atom_labels] for a in mol.GetAtoms()], dtype=np.int32)\n","    padding = np.zeros((max_length - features.shape[0], features.shape[1]))\n","    return np.vstack((features, padding))\n","\n","def feature_size(mol, atom_labels, max_atoms=60):\n","    \"\"\"\n","    Create a tensor of size (max_atoms x len(atom_labels)) with one-hot encoding for each atom.\n","    \"\"\"\n","    features = torch.zeros((max_atoms, len(atom_labels)))\n","    for i, atom in enumerate(mol.GetAtoms()):\n","        if i >= max_atoms:\n","            break\n","        try:\n","            idx = atom_labels.index(atom.GetAtomicNum())\n","        except ValueError:\n","            continue  # In case the atomic number is not in atom_labels.\n","        features[i, idx] = 1\n","    return features\n","\n","def graph_adjacency(mol, max_atoms=60, bond_encoder=None):\n","    \"\"\"\n","    Create an adjacency matrix (as a torch tensor) for the molecule.\n","    Each edge is weighted by the encoded bond type.\n","    \"\"\"\n","    bond_encoder = bond_encoder or {}\n","    adjacency = torch.zeros((max_atoms, max_atoms))\n","    for bond in mol.GetBonds():\n","        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n","        if start < max_atoms and end < max_atoms:\n","            weight = bond_encoder.get(bond.GetBondType(), 0)\n","            adjacency[start, end] = weight\n","            adjacency[end, start] = weight\n","    return adjacency\n","\n","def find_paths(G, u, n):\n","    \"\"\"\n","    Recursively find all simple paths of length n from node u in graph G.\n","    \"\"\"\n","    if n == 0:\n","        return [[u]]\n","    return [\n","        [u] + path\n","        for neighbor in G.neighbors(u)\n","        for path in find_paths(G, neighbor, n - 1)\n","        if u not in path\n","    ]\n","\n","def find_minimum_indices(arr):\n","    \"\"\"\n","    Return the indices where the minimum value occurs in the list.\n","    \"\"\"\n","    min_val = min(arr)\n","    return [index for index, value in enumerate(arr) if value == min_val]\n","\n","def remove_unique_classes(class_matrix, feature_matrix):\n","    \"\"\"\n","    Remove rows from class_matrix and feature_matrix where the class is unique.\n","    \"\"\"\n","    unique_classes, class_counts = np.unique(class_matrix, axis=0, return_counts=True)\n","    unique_indices = np.where(class_counts == 1)[0]\n","    return np.delete(class_matrix, unique_indices, axis=0), np.delete(feature_matrix, unique_indices, axis=0)\n","\n","def map_all_edges_g_to_G(g, G):\n","    \"\"\"\n","    Map DGL graph edges to the original NetworkX graph node names.\n","    \"\"\"\n","    edge_to_str_name_mapping = {}\n","    for edge_id in range(g.number_of_edges()):\n","        src_id, dst_id = g.find_edges(edge_id)\n","        src_name = g.ndata['name'][src_id].item()\n","        dst_name = g.ndata['name'][dst_id].item()\n","        src_in_G, dst_in_G = None, None\n","        for node, data in G.nodes(data=True):\n","            if data.get('name') == src_name:\n","                src_in_G = node\n","            if data.get('name') == dst_name:\n","                dst_in_G = node\n","            if src_in_G is not None and dst_in_G is not None:\n","                break\n","        if src_in_G is not None and dst_in_G is not None:\n","            edge_to_str_name_mapping[edge_id] = (src_in_G, dst_in_G)\n","    return edge_to_str_name_mapping\n","\n","# =============================================================================\n","# Data Extraction & Graph Construction Function\n","# =============================================================================\n","\n","def DataExtN():\n","    df = pd.read_excel('database.xlsx', 'BCPs')\n","    dfH = pd.read_excel('database.xlsx', 'Homopolymers')\n","    print('- Total BCP data:', df.shape[0])\n","    print('- Total Homopolymer data:', dfH.shape[0])\n","\n","    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","    # XE = [df['σbreak (MPa)']]\n","    # XE1 = [df['εbreak (%)']]\n","    XE = [df['εbreak (%)']]\n","    XE1 = [df['εbreak (%)']]\n","\n","    smiles = df['Big_Smile'].tolist()\n","    counts = df['Block DP']\n","    features, features1, features2 = [], [], []\n","    G = nx.DiGraph()\n","\n","    # Extract features for each SMILES entry\n","    for i in range(len(smiles)):\n","        features.append([XE[0][i]])\n","        features1.append([XE[0][i]])\n","        features2.append([XE1[0][i]])\n","    features = imputer.fit_transform(features)\n","\n","    Count = 0\n","    for i in range(len(smiles)):\n","        Sp = smiles[i][1:-1].split('}{')\n","        Cn = str(counts[i]).split(':')\n","        if len(Cn[0].split('/')) == 1:\n","            Cn[0] = str(2 * float(Cn[0]))\n","        else:\n","            AC = Cn[0].split('/')\n","            Cn[0] = str(2 * float(AC[0])) + '/' + str(2 * float(AC[1]))\n","\n","        # Expecting two segments\n","        Sp = [Sp[0], Sp[1]]\n","        Spp = []\n","        for l in range(2):\n","            if len(Sp[l].split(',')) == 1:\n","                Spp.append(Sp[l])\n","\n","        data = [Chem.MolFromSmiles(line) for line in Spp if Chem.MolFromSmiles(line) is not None]\n","        if not data:\n","            continue  # Skip if no valid molecule is generated\n","        atom_labels = sorted(set([atom.GetAtomicNum() for mol in data for atom in mol.GetAtoms()] + [0]))\n","        bond_labels = [Chem.rdchem.BondType.ZERO] + list(sorted(set(bond.GetBondType() for mol in data for bond in mol.GetBonds())))\n","        bond_encoder_m = {l: ii for ii, l in enumerate(bond_labels)}\n","\n","        Feat = []\n","        for idx, mol in enumerate(data):\n","            combined_feature = torch.cat([feature_size(mol, atom_labels),\n","                                          graph_adjacency(mol, bond_encoder=bond_encoder_m)], dim=1)\n","            DP_values = Cn[idx].split('/')\n","            dp_weight = torch.sqrt(torch.tensor(float(DP_values[0]), dtype=torch.float))\n","            weighted_features = (combined_feature + 1) * dp_weight\n","            Feat.append(weighted_features.float())\n","\n","        Feat = torch.stack(Feat)\n","        Feat = F.pad(Feat, (0, 65 - Feat.size(-1)), \"constant\", 0)\n","\n","        # Process sub-features if any segment has a comma\n","        if len(Sp[0].split(',')) > 1 or len(Sp[1].split(',')) > 1:\n","            if len(Sp[0].split(',')) > 1:\n","                ss = Sp[0].split(',')\n","                jj = Cn[0].split('/')\n","            if len(Sp[1].split(',')) > 1:\n","                ss = Sp[1].split(',')\n","                jj = Cn[1].split('/')\n","            data = [Chem.MolFromSmiles(ss[0])]\n","            atom_labels = sorted(set([atom.GetAtomicNum() for mol in data for atom in mol.GetAtoms()] + [0]))\n","            bond_labels = [Chem.rdchem.BondType.ZERO] + list(sorted(set(bond.GetBondType() for mol in data for bond in mol.GetBonds())))\n","            bond_encoder_m = {l: ii for ii, l in enumerate(bond_labels)}\n","            Feat1 = []\n","            for idx, mol in enumerate(data):\n","                combined_feature = torch.cat([feature_size(mol, atom_labels),\n","                                              graph_adjacency(mol, bond_encoder=bond_encoder_m)], dim=1)\n","                dp_weight = torch.sqrt(torch.tensor(float(jj[0]), dtype=torch.float))\n","                weighted_features = (combined_feature + 1) * dp_weight\n","                Feat1.append(weighted_features.float())\n","            Feat1 = torch.stack(Feat1)\n","            Feat1 = F.pad(Feat1, (0, 65 - Feat1.size(-1)), \"constant\", 0)\n","\n","            data = [Chem.MolFromSmiles(ss[1])]\n","            atom_labels = sorted(set([atom.GetAtomicNum() for mol in data for atom in mol.GetAtoms()] + [0]))\n","            bond_labels = [Chem.rdchem.BondType.ZERO] + list(sorted(set(bond.GetBondType() for mol in data for bond in mol.GetBonds())))\n","            bond_encoder_m = {l: ii for ii, l in enumerate(bond_labels)}\n","            Feat2 = []\n","            for idx, mol in enumerate(data):\n","                combined_feature = torch.cat([feature_size(mol, atom_labels),\n","                                              graph_adjacency(mol, bond_encoder=bond_encoder_m)], dim=1)\n","                dp_weight = torch.sqrt(torch.tensor(float(jj[1]), dtype=torch.float))\n","                weighted_features = (combined_feature + 1) * dp_weight\n","                Feat2.append(weighted_features.float())\n","            Feat2 = torch.stack(Feat2)\n","            Feat2 = F.pad(Feat2, (0, 65 - Feat2.size(-1)), \"constant\", 0)\n","            Feat3 = Feat1 + Feat2\n","            Feat = torch.cat((Feat, Feat3), dim=0)\n","\n","        # Create padded features for the node (flatten and pad to fixed length 4000)\n","        padded_features = []\n","        for feat in Feat:\n","            feat_flat = feat.reshape(-1)\n","            pad_len = 4000 - feat_flat.shape[0]\n","            feat_flat = torch.cat([feat_flat, torch.zeros(pad_len)])\n","            padded_features.append(feat_flat)\n","        padded_features = torch.stack(padded_features)\n","        # --- FIX: Ensure we have at least two feature vectors ---\n","        if padded_features.shape[0] < 2:\n","            padded_features = torch.cat([padded_features, padded_features], dim=0)\n","\n","        Feat_T = torch.tensor(features[i])\n","        Feat_TE = torch.tensor(features1[i])\n","        Feat_TE1 = torch.tensor(features2[i])\n","\n","        value0 = get_hard_soft_value(Sp[0], dfH)\n","        value1 = get_hard_soft_value(Sp[1], dfH)\n","\n","        node_name0 = Sp[0] + ';' + Cn[0]\n","        node_name1 = Sp[1] + ';' + (Cn[1] if len(Cn) > 1 else Cn[0])\n","\n","        if value0 is not None and value1 is None:\n","            if not G.has_node(node_name0):\n","                Count += 1\n","                G.add_node(node_name0, Feature=padded_features[0],\n","                           weight=Feat_T, bipartite=value0,\n","                           weightBreak=Feat_TE1, name=Count)\n","            if not G.has_node(node_name1):\n","                Count += 1\n","                G.add_node(node_name1, Feature=padded_features[1],\n","                           weight=Feat_T, bipartite=1 - value0,\n","                           weightBreak=Feat_TE1, name=Count)\n","        elif value1 is not None and value0 is None:\n","            if not G.has_node(node_name0):\n","                Count += 1\n","                G.add_node(node_name0, Feature=padded_features[0],\n","                           weight=Feat_T, bipartite=1 - value1,\n","                           weightBreak=Feat_TE1, name=Count)\n","            if not G.has_node(node_name1):\n","                Count += 1\n","                G.add_node(node_name1, Feature=padded_features[1],\n","                           weight=Feat_T, bipartite=value1,\n","                           weightBreak=Feat_TE1, name=Count)\n","        elif value0 is not None and value1 is not None:\n","            if not G.has_node(node_name0):\n","                Count += 1\n","                G.add_node(node_name0, Feature=padded_features[0],\n","                           weight=Feat_T, bipartite=value0,\n","                           weightBreak=Feat_TE1, name=Count)\n","            if not G.has_node(node_name1):\n","                Count += 1\n","                G.add_node(node_name1, Feature=padded_features[1],\n","                           weight=Feat_T, bipartite=value1,\n","                           weightBreak=Feat_TE1, name=Count)\n","        else:\n","            if not G.has_node(node_name0):\n","                Count += 1\n","                G.add_node(node_name0, Feature=padded_features[0],\n","                           weight=Feat_T, bipartite=0,\n","                           weightBreak=Feat_TE1, name=Count)\n","            if not G.has_node(node_name1):\n","                Count += 1\n","                G.add_node(node_name1, Feature=padded_features[1],\n","                           weight=Feat_T, bipartite=1,\n","                           weightBreak=Feat_TE1, name=Count)\n","\n","        G.add_edge(node_name0, node_name1, Feature=padded_features[0],\n","                   weight=Feat_TE, weightBreak=Feat_TE1)\n","\n","    mapping = {node: i for i, node in enumerate(G.nodes())}\n","    reverse_mapping = {i: node for node, i in mapping.items()}\n","    dgl_g = dgl.from_networkx(G, node_attrs=['Feature','weight','weightBreak','bipartite','name'],\n","                              edge_attrs=['Feature','weight','weightBreak'])\n","    return dgl_g, G, reverse_mapping\n","\n","\n","# =============================================================================\n","# Graph Neural Network Model Definitions\n","# =============================================================================\n","\n","class GraphSAGE(nn.Module):\n","    def __init__(self, in_feats, h_feats):\n","        super(GraphSAGE, self).__init__()\n","        self.conv1 = dgl.nn.SAGEConv(in_feats, h_feats, 'mean')\n","        self.dropout = nn.Dropout(0.5)  # Dropout rate 0.5\n","        self.conv2 = dgl.nn.SAGEConv(h_feats, h_feats, 'mean')\n","        self.conv3 = dgl.nn.SAGEConv(h_feats, h_feats, 'mean')\n","        self.conv4 = dgl.nn.SAGEConv(h_feats, h_feats, 'mean')\n","\n","    def forward(self, g, in_feat):\n","        h = self.conv1(g, in_feat)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        h = self.conv2(g, h)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        h = self.conv3(g, h)\n","        h = F.relu(h)\n","        h = self.dropout(h)\n","        h = self.conv4(g, h)\n","        return h\n","\n","class DotPredictor(nn.Module):\n","    def forward(self, g, h):\n","        g.ndata['h'] = h\n","        g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n","        # Return scores as a 1D tensor along with the updated graph\n","        return g.edata['score'][:, 0], g\n","\n","def init_weights(m):\n","    if isinstance(m, nn.Linear):\n","        nn.init.xavier_uniform_(m.weight)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0.01)\n","    elif hasattr(m, 'fc_self'):\n","        nn.init.xavier_uniform_(m.fc_self.weight)\n","        if m.fc_self.bias is not None:\n","            m.fc_self.bias.data.fill_(0.01)\n","    elif hasattr(m, 'fc_neigh'):\n","        nn.init.xavier_uniform_(m.fc_neigh.weight)\n","        if m.fc_neigh.bias is not None:\n","            m.fc_neigh.bias.data.fill_(0.01)\n","\n","def compute_loss(pos_score, neg_score, weight_pos=1.0, weight_neg=1.0):\n","    pos_loss = F.binary_cross_entropy_with_logits(\n","        pos_score, torch.ones_like(pos_score),\n","        weight=torch.full_like(pos_score, weight_pos)\n","    )\n","    neg_loss = F.binary_cross_entropy_with_logits(\n","        neg_score, torch.zeros_like(neg_score),\n","        weight=torch.full_like(neg_score, weight_neg)\n","    )\n","    return pos_loss + neg_loss\n","\n","def compute_auc(pos_score, neg_score):\n","    scores = torch.cat([pos_score, neg_score]).detach().cpu().numpy()\n","    labels = torch.cat([torch.ones(pos_score.shape[0]),\n","                        torch.zeros(neg_score.shape[0])]).detach().cpu().numpy()\n","    return roc_auc_score(labels, scores)\n","\n","def calculate_rmse(pred_scores, true_weights):\n","    mse = np.mean((pred_scores.numpy() - true_weights.numpy()) ** 2)\n","    return math.sqrt(mse)\n","\n","# =============================================================================\n","# Main Training and Evaluation Routine\n","# =============================================================================\n","\n","def main():\n","    seed = 2\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Load the data and construct graphs\n","    g, G, reverse_mapping = DataExtN()\n","    u, v = g.edges()\n","    eids = np.arange(g.number_of_edges())\n","\n","    # Generate negative edges using bipartite node subset\n","    bipartite_nodes = np.where(g.ndata[\"bipartite\"] == 1)[0]\n","    current_edges = set(zip(u.numpy(), v.numpy()))\n","    possible_neg_edges = [(x, y) for x in bipartite_nodes for y in bipartite_nodes if x != y]\n","    neg_edges = [edge for edge in possible_neg_edges if edge not in current_edges]\n","    if not neg_edges:\n","        raise ValueError(\"No negative edges found. Check your bipartite node definitions.\")\n","    neg_u, neg_v = zip(*neg_edges)\n","\n","    # Set up 2-fold cross-validation\n","    kf = KFold(n_splits=2, shuffle=True, random_state=seed)\n","\n","    for fold, (train_val_index, test_index) in enumerate(kf.split(eids)):\n","        train_index, val_index = train_test_split(train_val_index, test_size=0.5, random_state=seed)\n","        train_size = len(train_index)\n","        val_size = len(val_index)\n","        test_size = len(test_index)\n","\n","        # Select positive edges for train/validation/test\n","        train_pos_u, train_pos_v = u[train_index], v[train_index]\n","        val_pos_u, val_pos_v = u[val_index], v[val_index]\n","        test_pos_u, test_pos_v = u[test_index], v[test_index]\n","\n","        # Sample negative edges to match the number of positive edges\n","        neg_indices = np.random.permutation(len(neg_u))\n","        train_neg_u = np.array(neg_u)[neg_indices[:train_size]]\n","        train_neg_v = np.array(neg_v)[neg_indices[:train_size]]\n","        val_neg_u = np.array(neg_u)[neg_indices[train_size:train_size + val_size]]\n","        val_neg_v = np.array(neg_v)[neg_indices[train_size:train_size + val_size]]\n","        test_neg_u = np.array(neg_u)[neg_indices[train_size + val_size:train_size + val_size + test_size]]\n","        test_neg_v = np.array(neg_v)[neg_indices[train_size + val_size:train_size + val_size + test_size]]\n","\n","        # Create training graph by removing a subset of edges from the full graph\n","        train_g = dgl.remove_edges(g, eids[:train_size + val_size])\n","\n","        # Build subgraphs for positive edges\n","        def build_subgraph(pos_u, pos_v):\n","            sub_g = dgl.graph((pos_u, pos_v), num_nodes=g.number_of_nodes())\n","            sub_g.ndata.update(g.ndata)\n","            return sub_g\n","\n","        train_pos_g = build_subgraph(train_pos_u, train_pos_v)\n","        val_pos_g = build_subgraph(val_pos_u, val_pos_v)\n","        test_pos_g = build_subgraph(test_pos_u, test_pos_v)\n","\n","        # Build subgraphs for negative edges\n","        train_neg_g = dgl.graph((train_neg_u, train_neg_v), num_nodes=g.number_of_nodes())\n","        train_neg_g.ndata.update(g.ndata)\n","        val_neg_g = dgl.graph((val_neg_u, val_neg_v), num_nodes=g.number_of_nodes())\n","        val_neg_g.ndata.update(g.ndata)\n","        test_neg_g = dgl.graph((test_neg_u, test_neg_v), num_nodes=g.number_of_nodes())\n","        test_neg_g.ndata.update(g.ndata)\n","\n","        # Initialize model and predictor\n","        in_feats = g.ndata['Feature'].shape[1]\n","        model = GraphSAGE(in_feats, 32)\n","        model.apply(init_weights)\n","        pred = DotPredictor()\n","\n","        optimizer = torch.optim.Adam(\n","            itertools.chain(model.parameters(), pred.parameters()),\n","            lr=0.001, weight_decay=1e-4\n","        )\n","        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=10)\n","\n","        best_val_loss = float('inf')\n","        patience = 1000\n","        no_improvement_count = 0\n","\n","        # Training loop\n","        for e in range(500):\n","            model.train()\n","            h = model(train_g, train_g.ndata['Feature'].float())\n","            pos_score, _ = pred(train_pos_g, h)\n","            neg_score, _ = pred(train_neg_g, h)\n","            loss = compute_loss(pos_score, neg_score)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            model.eval()\n","            with torch.no_grad():\n","                val_h = model(train_g, train_g.ndata['Feature'].float())\n","                val_pos_score, _ = pred(val_pos_g, val_h)\n","                val_neg_score, _ = pred(val_neg_g, val_h)\n","                val_loss = compute_loss(val_pos_score, val_neg_score)\n","\n","            if val_loss < best_val_loss:\n","                best_val_loss = val_loss\n","                torch.save(model.state_dict(), \"best_model.pth\")\n","                no_improvement_count = 0\n","            else:\n","                no_improvement_count += 1\n","\n","            if e % 20 == 0:\n","                print(f'Epoch {e}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n","\n","            if no_improvement_count >= patience:\n","                print(\"Early stopping due to no improvement\")\n","                break\n","\n","        # Evaluation on test set\n","        model.eval()\n","        with torch.no_grad():\n","            test_h = model(test_pos_g, test_pos_g.ndata['Feature'].float())\n","            pos_score, _ = pred(test_pos_g, test_h)\n","            neg_score, _ = pred(test_neg_g, test_h)\n","            auc = compute_auc(pos_score, neg_score)\n","            print(f'Fold {fold + 1}, AUC: {auc:.4f}')\n","\n","            # Predict on the full graph and map edges back to original names\n","            full_score, gF = pred(g, test_h)\n","            edge_to_str_name_mapping = map_all_edges_g_to_G(gF, G)\n","\n","            predicted_scores = []\n","            true_weights = []\n","            with open('predicted_edges.txt', 'w') as f:\n","                f.write(\"Source,Target,Score,Weight\\n\")\n","                for edge_id, (src_name, dst_name) in edge_to_str_name_mapping.items():\n","                    predicted_score = gF.edges[edge_id].data['score'].item()\n","                    true_weight = gF.edges[edge_id].data['weight'].item()\n","                    predicted_scores.append(predicted_score)\n","                    true_weights.append(true_weight)\n","                    f.write(f\"{src_name},{dst_name},{predicted_score},{true_weight}\\n\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","source":["predicted_scores = []\n","true_weights = []\n","\n","# Save predicted edges (for inspection) and collect scores and weights\n","with open('predicted_edges.txt', 'w') as f:\n","    f.write(\"Source,Target,Score,Weight\\n\")\n","    for edge_id, (src_name, dst_name) in edge_to_str_name_mapping.items():\n","        predicted_score = gF.edges[edge_id].data['score'].item()\n","        true_weight = gF.edges[edge_id].data['weight'].item()\n","        predicted_scores.append(predicted_score)\n","        true_weights.append(true_weight)\n","        f.write(f\"{src_name},{dst_name},{predicted_score},{true_weight}\\n\")\n","\n","# Define the threshold and tolerance\n","threshold = 800.0\n","tolerance = 0\n","\n","amb_lower = threshold - tolerance\n","amb_upper = threshold + tolerance\n","\n","fuzzy_correct = 0\n","for pred, true in zip(predicted_scores, true_weights):\n","    true_class = 1 if true > threshold else 0\n","    pred_class = 1 if pred > threshold else 0\n","\n","    if amb_lower <= pred <= amb_upper:\n","        fuzzy_correct += 1\n","    else:\n","        if pred_class == true_class:\n","            fuzzy_correct += 1\n","\n","fuzzy_accuracy = fuzzy_correct / len(true_weights)\n","print(f\"Classification Accuracy : {fuzzy_accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TrVP6LsaeuQ9","executionInfo":{"status":"ok","timestamp":1741525478485,"user_tz":0,"elapsed":57,"user":{"displayName":"Soheila Molaei","userId":"07707104183566116527"}},"outputId":"8f8f93f1-26c6-4a7a-b7bb-0e9a5d980f56"},"execution_count":179,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Accuracy : 1.0000\n"]}]},{"cell_type":"code","source":["!pip install dgl\n","!pip install rdkit"],"metadata":{"id":"9dScMowVcKCY"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"gRMkn4ik2zcJ","outputId":"fe9e0ba4-2788-452d-89c2-f0950dae335e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741512319961,"user_tz":0,"elapsed":2745,"user":{"displayName":"Soheila Molaei","userId":"07707104183566116527"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["- Total data: 404\n","404\n","*********************************\n"]}],"source":["def DataExtNGH():\n","  dfH = pd.read_excel('database.xlsx','Homopolymers')\n","  # dfH = pd.read_excel('homopolymers_overall_dp.xlsx')\n","  # df = df[df['Length'] <= args.size].reset_index(drop=True)\n","  print('- Total data:', dfH.shape[0])\n","\n","  # try:\n","  #     df = df.sample(n=111).reset_index(drop=True)\n","  #     # dfH = dfH.sample(n=285).reset_index(drop=True)\n","  #     print('- Sampled data:', df.shape[0])\n","  # except:\n","  #     print(f'Sampling error: Set the value of --data lower than {df.shape[0]}.')\n","  #     quit()\n","\n","  import numpy as np\n","\n","  imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n","\n","  #*************************************************************************\n","  # #Normalize Part\n","  # XE=df ['σbreak (MPa)']\n","  # XE1=df ['εbreak (%)']\n","  # # Normalize 'σbreak (MPa)'\n","  # XE_normalized = (XE - XE.min()) / (XE.max() - XE.min())\n","  # # Normalize 'εbreak (%)'\n","  # XE1_normalized = (XE1 - XE1.min()) / (XE1.max() - XE1.min())\n","  # print(XE1.min(),XE1.max())\n","  # XE=[XE_normalized]\n","  # XE1=[XE1_normalized]\n","  # #*************************************************************************\n","\n","  # df['Lower_Tg'], df['Upper_Tg']=X[0],X[1]\n","  Sm=[]\n","  Cl=[]\n","  k=-1\n","\n","  features =[]\n","  features1=[]\n","  features2=[]\n","  import numpy as np\n","\n","  GH=nx.DiGraph()\n","\n","\n","  #******************************\n","  SmilesHo=dfH['Big_Smile'].tolist()\n","  DP=dfH['Overall DP'].tolist()\n","  HrdSoft=dfH['Hard/Soft'].tolist()\n","  Count=0\n","  print(len(SmilesHo))\n","  print(\"*********************************\")\n","  for i ,j,z in zip(SmilesHo[0:len(SmilesHo)], DP[0:len(SmilesHo)], HrdSoft[0:len(SmilesHo)]):\n","#   for i ,j,z in zip(SmilesHo[0:309], DP[0:309], HrdSoft[0:309]):\n","    # print(len(i.split(',')))\n","    # print(i,j)\n","    # print(Chem.MolFromSmiles(i))\n","    if len(i.split(','))==1:\n","\n","      data = [Chem.MolFromSmiles(i)]\n","      atom_labels = sorted(set([atom.GetAtomicNum() for mol in data for atom in mol.GetAtoms()] + [0]))\n","      bond_labels = [Chem.rdchem.BondType.ZERO] + list(sorted(set(bond.GetBondType() for mol in data for bond in mol.GetBonds())))\n","      bond_encoder_m = {l: ii for ii, l in enumerate(bond_labels)}\n","\n","      Feat = []\n","      for idx, mol in enumerate(data): # Use enumerate to get the index of the molecule\n","          combined_feature = torch.cat([feature_size(mol, atom_labels), graph_adjacency(mol, bond_encoder=bond_encoder_m)], 1)\n","\n","          # Weight the features by the square root of the DP value\n","          DP_values=j\n","\n","          dp_weight = torch.sqrt(torch.tensor(float(DP_values), dtype=torch.float))\n","        #   print(float(DP_values))\n","          weighted_features = (combined_feature+1) * dp_weight\n","\n","          Feat.append(weighted_features.float())\n","      Feat = torch.stack(Feat)\n","    else:\n","    #   print(i,j)\n","      ss=i.split(',')\n","      jj=j.split('/')\n","      data = [Chem.MolFromSmiles(ss[0])]\n","      atom_labels = sorted(set([atom.GetAtomicNum() for mol in data for atom in mol.GetAtoms()] + [0]))\n","      bond_labels = [Chem.rdchem.BondType.ZERO] + list(sorted(set(bond.GetBondType() for mol in data for bond in mol.GetBonds())))\n","      bond_encoder_m = {l: ii for ii, l in enumerate(bond_labels)}\n","      Feat1 = []\n","      for idx, mol in enumerate(data): # Use enumerate to get the index of the molecule\n","          combined_feature = torch.cat([feature_size(mol, atom_labels), graph_adjacency(mol, bond_encoder=bond_encoder_m)], 1)\n","\n","          # Weight the features by the square root of the DP value\n","          DP_values=jj[0]\n","          dp_weight = torch.sqrt(torch.tensor(float(DP_values), dtype=torch.float))\n","          weighted_features = (combined_feature+1) * dp_weight\n","\n","          Feat1.append(weighted_features.float())\n","\n","      data = [Chem.MolFromSmiles(ss[1])]\n","      atom_labels = sorted(set([atom.GetAtomicNum() for mol in data for atom in mol.GetAtoms()] + [0]))\n","      bond_labels = [Chem.rdchem.BondType.ZERO] + list(sorted(set(bond.GetBondType() for mol in data for bond in mol.GetBonds())))\n","      bond_encoder_m = {l: ii for ii, l in enumerate(bond_labels)}\n","      Feat2 = []\n","      for idx, mol in enumerate(data): # Use enumerate to get the index of the molecule\n","          combined_feature = torch.cat([feature_size(mol, atom_labels), graph_adjacency(mol, bond_encoder=bond_encoder_m)], 1)\n","\n","          # Weight the features by the square root of the DP value\n","          DP_values=jj[1]\n","          dp_weight = torch.sqrt(torch.tensor(float(DP_values), dtype=torch.float))\n","          weighted_features = (combined_feature+1) * dp_weight\n","\n","          Feat2.append(weighted_features.float())\n","      Feat=torch.stack(Feat1)+torch.stack(Feat2)\n","      # Feat= torch.cat((torch.stack(Feat1), torch.stack(Feat2)), dim=2)\n","\n","\n","    padded_features = []\n","    # print(Feat.shape)\n","    Feat_M = torch.reshape(Feat, (-1,))\n","    # print(Feat.shape)\n","\n","    # print(\"feat.shape\",Feat_M.shape)\n","    padding_length = 4000 - Feat_M.shape[0]\n","    Feat_M_padded = torch.cat([Feat_M, torch.zeros(padding_length)])\n","\n","    padded_features.append(Feat_M_padded)\n","    ss=0\n","    padded_features = torch.stack(padded_features)\n","    Count=Count+1\n","    padded_features = torch.reshape(padded_features, (-1,))\n","\n","    # print(padded_features.shape)\n","    GH.add_node(i+';'+str(j) ,Feature=padded_features,bipartite=int(z),name=Count)\n","\n","\n","\n","\n","  # Create a mapping from the original string labels to integers\n","  mapping = {node: i for i, node in enumerate(G.nodes())}\n","  # Save the reverse mapping to map back to string labels later if needed\n","  reverse_mapping = {i: node for node, i in mapping.items()}\n","\n","  # G_combined = nx.compose(G, GH)\n","  gH=dgl.from_networkx(GH, node_attrs=['Feature','bipartite','name'])\n","  BH=list(GH.nodes())\n","\n","  # print(gH.ndata['Feature'].shape)\n","  return reverse_mapping,gH,GH\n","reverse_mapping,gH,GH=DataExtNGH()"]},{"cell_type":"code","source":["def combine_graphs(g, gH):\n","    num_nodes_g = g.number_of_nodes()\n","    num_nodes_gH = gH.number_of_nodes()\n","\n","    # Check for missing names in g\n","    print(\"Nodes without names in g:\", [i for i in range(g.number_of_nodes()) if 'name' not in g.ndata or g.ndata['name'][i] == ''])\n","\n","    # Check for missing names in gH\n","    print(\"Nodes without names in gH:\", [i for i in range(gH.number_of_nodes()) if 'name' not in gH.ndata or gH.ndata['name'][i] == ''])\n","\n","    # Get edges from g\n","    src_g, dst_g = g.edges()\n","\n","    # Get edges from gH and shift the node IDs to avoid overlap with g\n","    src_gH, dst_gH = gH.edges()\n","    src_gH = src_gH + num_nodes_g  # Shift gH's node indices\n","    dst_gH = dst_gH + num_nodes_g  # Shift gH's node indices\n","\n","    # Combine the edges of g and gH\n","    combined_src = torch.cat([src_g, src_gH])\n","    combined_dst = torch.cat([dst_g, dst_gH])\n","\n","    # Create the combined graph\n","    combined_graph = dgl.graph((combined_src, combined_dst), num_nodes=num_nodes_g + num_nodes_gH)\n","\n","    # Combine node features (e.g., 'Feature', 'bipartite', etc.)\n","    if 'Feature' in g.ndata and 'Feature' in gH.ndata:\n","        combined_graph.ndata['Feature'] = torch.cat([g.ndata['Feature'], gH.ndata['Feature']], dim=0)\n","\n","    if 'bipartite' in g.ndata and 'bipartite' in gH.ndata:\n","        combined_graph.ndata['bipartite'] = torch.cat([g.ndata['bipartite'], gH.ndata['bipartite']], dim=0)\n","\n","    # If names exist, combine node names\n","    if 'name' in g.ndata and 'name' in gH.ndata:\n","        combined_graph.ndata['name'] = torch.cat([g.ndata['name'], gH.ndata['name']], dim=0)\n","\n","    return combined_graph"],"metadata":{"id":"2lHPzOHD9Kql","executionInfo":{"status":"ok","timestamp":1741512321924,"user_tz":0,"elapsed":2,"user":{"displayName":"Soheila Molaei","userId":"07707104183566116527"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":15,"metadata":{"id":"GvwmxTpY2zcK","outputId":"1595f60d-1715-4ad0-f585-2973b88ecf01","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741512432624,"user_tz":0,"elapsed":68369,"user":{"displayName":"Soheila Molaei","userId":"07707104183566116527"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Nodes without names in g: []\n","Nodes without names in gH: []\n","Number of nodes in combined graph: 549\n","Number of possible links generated: 150426\n","Number of predicted links above threshold: 271039\n","Number of predicted links: 271039\n","Number of mapped edges (including predictions): 271039\n","Number of edges in the new graph with predicted links: 135424\n","Predicted links between homopolymers saved to 'edges.txt'.\n"]}],"source":["#This one is link prediction with threshold for link part ---Last version!!!!\n","\n","import itertools\n","import torch\n","import dgl\n","from torch.nn import Module\n","from torch.nn.functional import cosine_similarity\n","import dgl.function as fn\n","\n","# Function to combine graphs g and gH into a single graph\n","def combine_graphs(g, gH):\n","    num_nodes_g = g.number_of_nodes()\n","    num_nodes_gH = gH.number_of_nodes()\n","\n","    # Check for missing names in g\n","    print(\"Nodes without names in g:\", [i for i in range(g.number_of_nodes()) if 'name' not in g.ndata or g.ndata['name'][i] == ''])\n","\n","    # Check for missing names in gH\n","    print(\"Nodes without names in gH:\", [i for i in range(gH.number_of_nodes()) if 'name' not in gH.ndata or gH.ndata['name'][i] == ''])\n","\n","    # Get edges from g\n","    src_g, dst_g = g.edges()\n","\n","    # Get edges from gH and shift the node IDs to avoid overlap with g\n","    src_gH, dst_gH = gH.edges()\n","    src_gH = src_gH + num_nodes_g  # Shift gH's node indices\n","    dst_gH = dst_gH + num_nodes_g  # Shift gH's node indices\n","\n","    # Combine the edges of g and gH\n","    combined_src = torch.cat([src_g, src_gH])\n","    combined_dst = torch.cat([dst_g, dst_gH])\n","\n","    # Create the combined graph\n","    combined_graph = dgl.graph((combined_src, combined_dst), num_nodes=num_nodes_g + num_nodes_gH)\n","\n","    # Combine node features (e.g., 'Feature', 'bipartite', etc.)\n","    if 'Feature' in g.ndata and 'Feature' in gH.ndata:\n","        combined_graph.ndata['Feature'] = torch.cat([g.ndata['Feature'], gH.ndata['Feature']], dim=0)\n","\n","    if 'bipartite' in g.ndata and 'bipartite' in gH.ndata:\n","        combined_graph.ndata['bipartite'] = torch.cat([g.ndata['bipartite'], gH.ndata['bipartite']], dim=0)\n","\n","    # If names exist, combine node names\n","    if 'name' in g.ndata and 'name' in gH.ndata:\n","        combined_graph.ndata['name'] = torch.cat([g.ndata['name'], gH.ndata['name']], dim=0)\n","\n","    return combined_graph\n","\n","# Function to generate all possible links within the combined graph\n","# Function to generate all possible links in the combined graph\n","import itertools\n","\n","# Function to generate all possible links within the combined graph\n","def generate_all_possible_links(combined_g, directed=False):\n","    num_nodes = combined_g.number_of_nodes()\n","\n","    if directed:\n","        # For directed edges, use product to include self-loops\n","        possible_links = list(itertools.product(range(num_nodes), repeat=2))\n","    else:\n","        # For undirected edges, use combinations to exclude self-loops\n","        possible_links = list(itertools.combinations(range(num_nodes), 2))\n","\n","    return possible_links\n","\n","\n","\n","class DotPredictor(Module):\n","    def forward(self, g, h, threshold=0.5):\n","        g.ndata['h'] = h\n","        g.apply_edges(fn.u_dot_v('h', 'h', 'score'))  # Using dot product for prediction score\n","        scores = g.edata['score'][:, 0]\n","\n","        # Filter edges based on threshold\n","        valid_edges = (scores > threshold).nonzero().squeeze()\n","        src, dst = g.edges(valid_edges)\n","        return src, dst, scores[valid_edges]\n","\n","\n","# Option: Using cosine similarity for scoring with threshold\n","def cosine_predictor(h, possible_links, threshold=0.5):\n","    predicted_links = []\n","    for src, dst in possible_links:\n","        score = cosine_similarity(h[src], h[dst], dim=0).item()  # Cosine similarity score\n","        if score > threshold:  # Apply threshold here\n","            predicted_links.append((src, dst, score))\n","    return predicted_links\n","\n","\n","# Function to predict links for the combined graph\n","def predict_new_links(combined_g, possible_links, model, threshold=0.5):\n","    with torch.no_grad():\n","        h = model(combined_g, combined_g.ndata['Feature'].float())  # Get embeddings from the model\n","\n","        # Pass the threshold for filtering low-confidence links\n","        predicted_links = cosine_predictor(h, possible_links, threshold)\n","        print(f\"Number of predicted links above threshold: {len(predicted_links)}\")\n","        return predicted_links\n","\n","\n","# Function to map the predicted edges back to node names\n","def map_predicted_edges(combined_g, G_combined, predicted_links):\n","    edge_to_str_name_mapping = {}\n","\n","    # Iterate through the predicted links and map them to their node names\n","    for src, dst, _ in predicted_links:\n","        src_name = combined_g.ndata['name'][src].item()\n","        dst_name = combined_g.ndata['name'][dst].item()\n","\n","        src_name_in_G = None\n","        dst_name_in_G = None\n","\n","        # Map the node names in combined_g back to G_combined\n","        for node, data in G_combined.nodes(data=True):\n","            if data.get('name') == src_name:\n","                src_name_in_G = node\n","            if data.get('name') == dst_name:\n","                dst_name_in_G = node\n","            if src_name_in_G is not None and dst_name_in_G is not None:\n","                break\n","\n","        # If both node names are found, store the mapping\n","        if src_name_in_G is not None and dst_name_in_G is not None:\n","            edge_to_str_name_mapping[(src, dst)] = (src_name_in_G, dst_name_in_G)\n","\n","    return edge_to_str_name_mapping\n","\n","\n","# Main workflow\n","# Combine the graphs g and gH\n","combined_g = combine_graphs(g, gH)\n","print(f\"Number of nodes in combined graph: {combined_g.number_of_nodes()}\")\n","# Generate all possible links within the combined graph\n","possible_links = generate_all_possible_links(combined_g)\n","print(f\"Number of possible links generated: {len(possible_links)}\")\n","\n","# Assuming the GraphSAGE model is trained and available as `model`\n","# Generate all possible links within the combined graph\n","possible_links = generate_all_possible_links(combined_g, directed=True)  # Adjust directed as needed\n","\n","# Assuming the GraphSAGE model is trained and available as `model`\n","# Predict new links\n","predicted_links = predict_new_links(combined_g, possible_links, model)\n","\n","# Mapping predicted edges back to their node names\n","G_combined = nx.compose(G, GH)\n","edge_to_str_name_mapping = map_predicted_edges(combined_g, G_combined, predicted_links)\n","\n","# Print or log to verify results\n","print(f\"Number of predicted links: {len(predicted_links)}\")\n","print(f\"Number of mapped edges (including predictions): {len(edge_to_str_name_mapping)}\")\n","\n","\n","# Now create the new graph with both original and predicted edges\n","# Extract the original edges from gH before combining the graphs\n","original_src, original_dst = gH.edges()\n","original_src = original_src.tolist()\n","original_dst = original_dst.tolist()\n","\n","# Extract predicted edges, adjusting indices back to gH's node range\n","predicted_edges = [\n","    (src - g.number_of_nodes(), dst - g.number_of_nodes())\n","    for src, dst, _ in predicted_links\n","    if src >= g.number_of_nodes() and dst >= g.number_of_nodes()\n","]\n","\n","# Combine original edges and predicted edges\n","new_src = original_src + [edge[0] for edge in predicted_edges]\n","new_dst = original_dst + [edge[1] for edge in predicted_edges]\n","\n","# Create a new graph with both original and predicted edges (number of nodes in gH)\n","gH_with_new_edges = dgl.graph((new_src, new_dst), num_nodes=gH.number_of_nodes())\n","\n","# Transfer the node features from the old graph to the new graph\n","gH_with_new_edges.ndata.update(gH.ndata)\n","\n","# (Optional) Print to verify if the new graph has the expected number of edges\n","print(f\"Number of edges in the new graph with predicted links: {gH_with_new_edges.number_of_edges()}\")\n","\n","\n","# Map predicted edges back to their names and save them\n","with open('edges.txt', 'a') as file:\n","    for edge_id, (str_name_u, str_name_v) in edge_to_str_name_mapping.items():\n","        src_id, dst_id = gH_with_new_edges.find_edges(edge_id)\n","        # if gH_with_new_edges.ndata['bipartite'][src_id] != 0 or gH_with_new_edges.ndata['bipartite'][dst_id] != 0:\n","            # Only print homopolymer-related edges\n","        if str_name_u.split(';')[0]!=str_name_v.split(';')[0]:\n","            file.write(f\"{str_name_u}* {str_name_v}\\n\")\n","# Remove this condition to save all predicted edges\n","# with open('edges.txt', 'a') as file:\n","#     for edge_id, (str_name_u, str_name_v) in edge_to_str_name_mapping.items():\n","#         file.write(f\"{str_name_u}* {str_name_v}\\n\")\n","\n","print(\"Predicted links between homopolymers saved to 'edges.txt'.\")\n","\n","\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}