{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62248518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# ========================= Tiny WL-subtree kernel (drop-in for GraKeL) =========================\n",
    "class SimpleWLKernel:\n",
    "    \"\"\"\n",
    "    Minimal WL-subtree kernel with normalize=True behavior.\n",
    "    API: fit_transform(train_graphs) -> K_train, transform(graphs) -> K_graphs_vs_train\n",
    "    Graphs are networkx.Graph with node attribute 'label' (string/int).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_iter=2, normalize=True):\n",
    "        self.n_iter = int(n_iter)\n",
    "        self.normalize = bool(normalize)\n",
    "        self._vocab = {}              # label -> column index (built on TRAIN)\n",
    "        self._train_feat = None       # [n_train, d]\n",
    "        self._frozen = False\n",
    "\n",
    "    @staticmethod\n",
    "    def _initial_labels(G):\n",
    "        labels = {}\n",
    "        for u, data in G.nodes(data=True):\n",
    "            lab = data.get('label', data.get('atom', None))\n",
    "            if lab is None:\n",
    "                lab = G.degree(u)\n",
    "            labels[u] = str(lab)\n",
    "        return labels\n",
    "\n",
    "    @staticmethod\n",
    "    def _relabel(G, labels):\n",
    "        new_labels = {}\n",
    "        for u in G.nodes():\n",
    "            multiset = sorted(labels[v] for v in G.neighbors(u))\n",
    "            new_labels[u] = f\"{labels[u]}|{'#'.join(multiset)}\"\n",
    "        return new_labels\n",
    "\n",
    "    def _featureize(self, graphs, build_vocab):\n",
    "        if build_vocab:\n",
    "            local_vocab = {}\n",
    "            # Pass 1: collect labels over all iterations and all graphs\n",
    "            all_bags = []  # list of list-of-dicts per graph\n",
    "            for G in graphs:\n",
    "                l = self._initial_labels(G)\n",
    "                bags = []\n",
    "                for _ in range(self.n_iter + 1):\n",
    "                    cnt = {}\n",
    "                    for u in G.nodes():\n",
    "                        lab = l[u]\n",
    "                        cnt[lab] = cnt.get(lab, 0) + 1\n",
    "                    bags.append(cnt)\n",
    "                    l = self._relabel(G, l)\n",
    "                all_bags.append(bags)\n",
    "                for b in bags:\n",
    "                    for lab in b.keys():\n",
    "                        if lab not in local_vocab:\n",
    "                            local_vocab[lab] = len(local_vocab)\n",
    "            # Pass 2: build fixed-length feature vectors\n",
    "            dim = len(local_vocab)\n",
    "            feat_rows = []\n",
    "            for bags in all_bags:\n",
    "                vec = [0] * dim\n",
    "                for b in bags:\n",
    "                    for lab, c in b.items():\n",
    "                        j = local_vocab.get(lab, None)\n",
    "                        if j is not None:\n",
    "                            vec[j] += c\n",
    "                feat_rows.append(vec)\n",
    "            return np.asarray(feat_rows, dtype=float), local_vocab\n",
    "        else:\n",
    "            # Vocab is frozen; single pass with existing indices\n",
    "            local_vocab = self._vocab\n",
    "            dim = len(local_vocab)\n",
    "            feat_rows = []\n",
    "            for G in graphs:\n",
    "                l = self._initial_labels(G)\n",
    "                vec = [0] * dim\n",
    "                for _ in range(self.n_iter + 1):\n",
    "                    cnt = {}\n",
    "                    for u in G.nodes():\n",
    "                        lab = l[u]\n",
    "                        cnt[lab] = cnt.get(lab, 0) + 1\n",
    "                    for lab, c in cnt.items():\n",
    "                        j = local_vocab.get(lab, None)\n",
    "                        if j is not None:\n",
    "                            vec[j] += c\n",
    "                    l = self._relabel(G, l)\n",
    "                feat_rows.append(vec)\n",
    "            return np.asarray(feat_rows, dtype=float), local_vocab\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_kernel(K):\n",
    "        diag = np.sqrt(np.clip(np.diag(K), 1e-12, None))\n",
    "        K = K / diag[:, None]\n",
    "        K = K / diag[None, :]\n",
    "        return K\n",
    "\n",
    "    @staticmethod\n",
    "    def _cross_normalize(F_all, F_train):\n",
    "        Ka = F_all @ F_train.T\n",
    "        na = np.sqrt(np.clip((F_all * F_all).sum(1, keepdims=True), 1e-12, None))\n",
    "        nb = np.sqrt(np.clip((F_train * F_train).sum(1, keepdims=True).T, 1e-12, None))\n",
    "        return Ka / (na @ nb)\n",
    "\n",
    "    def fit_transform(self, train_graphs):\n",
    "        F_train, vocab = self._featureize(train_graphs, build_vocab=True)\n",
    "        self._vocab = vocab\n",
    "        self._train_feat = F_train\n",
    "        self._frozen = True\n",
    "        K = F_train @ F_train.T\n",
    "        if self.normalize:\n",
    "            K = self._normalize_kernel(K)\n",
    "        return K\n",
    "\n",
    "    def transform(self, graphs):\n",
    "        if not self._frozen or self._train_feat is None:\n",
    "            raise RuntimeError(\"Call fit_transform on train graphs first.\")\n",
    "        F_all, _ = self._featureize(graphs, build_vocab=False)\n",
    "        if self.normalize:\n",
    "            return self._cross_normalize(F_all, self._train_feat)\n",
    "        else:\n",
    "            return F_all @ self._train_feat.T\n",
    "\n",
    "# ========================= Repro =========================\n",
    "def set_seed(seed=2):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(2)\n",
    "\n",
    "# ========================= Config =========================\n",
    "device = torch.device(\"cpu\")\n",
    "CHEMBERTA_DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "# Graph/encoder\n",
    "NEG_BASE_MULT = 10\n",
    "NEG_MAX_MULT  = 20\n",
    "HIDDEN_DIM    = 256\n",
    "WL_HEIGHT     = 2\n",
    "\n",
    "# Optional pretrained text embeddings\n",
    "CHEMBERTA_MODEL = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "CHEMBERTA_BATCH = 64\n",
    "USE_CHEMBERTA   = True  # set False to disable\n",
    "\n",
    "# Training lengths\n",
    "LP_PRETRAIN_EPOCHS = 600\n",
    "CLS_TRAIN_STEPS    = 1000\n",
    "\n",
    "# # Decision thresholds for tasks (engineering thresholds)\n",
    "SIGMA_THR   =15     # MPa\n",
    "EPSILON_THR =800    # %\n",
    "\n",
    "\n",
    "AUTO_TUNE_TOLERANCES = True\n",
    "SIGMA_TOL   = 0.0      # MPa   (manual σ tolerance)\n",
    "EPSILON_TOL = 0.0    # %     (manual ε tolerance)\n",
    "\n",
    "TRAIN_WITH_TOLERANT_LABELS = True\n",
    "\n",
    "# Calibrated confident bands (gentler to ensure nonzero coverage on tiny folds)\n",
    "CONF_HI = 0.60\n",
    "CONF_LO = 0.40\n",
    "\n",
    "# ========================= Small utils =========================\n",
    "def clean_smiles(smile: str) -> str:\n",
    "    s = str(smile).strip()\n",
    "    if (s.startswith('\"') and s.endswith('\"')) or (s.startswith(\"'\") and s.endswith(\"'\")):\n",
    "        s = s[1:-1]\n",
    "    if s.startswith('(') and s.endswith(')'):\n",
    "        s = s[1:-1]\n",
    "    if s.startswith('{') and s.endswith('}'):\n",
    "        s = s[1:-1]\n",
    "    return s.rstrip(',').strip()\n",
    "\n",
    "# Keep the original function name but implement with NetworkX to avoid changing the rest\n",
    "# of the code that calls `smiles_to_grakel_graph`.\n",
    "def smiles_to_grakel_graph(smile):\n",
    "    clean = clean_smiles(smile)\n",
    "    mol = Chem.MolFromSmiles(clean)\n",
    "    if mol is None:\n",
    "        print(f\"[SMILES ERROR] Could not parse: '{clean}'\")\n",
    "        return nx.Graph()\n",
    "    G = nx.Graph()\n",
    "    for i in range(mol.GetNumAtoms()):\n",
    "        sym = mol.GetAtomWithIdx(i).GetSymbol()\n",
    "        G.add_node(i, label=sym)\n",
    "    for b in mol.GetBonds():\n",
    "        u, v = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        G.add_edge(u, v)\n",
    "    return G\n",
    "\n",
    "def get_descriptors(smile):\n",
    "    mol = Chem.MolFromSmiles(clean_smiles(smile))\n",
    "    if mol is None:\n",
    "        return [0.0] * 5\n",
    "    return [\n",
    "        Descriptors.MolLogP(mol),\n",
    "        Descriptors.MolMR(mol),\n",
    "        Descriptors.TPSA(mol),\n",
    "        Descriptors.NumHAcceptors(mol),\n",
    "        Descriptors.NumHDonors(mol)\n",
    "    ]\n",
    "\n",
    "def joined_dp_text(dp_parts):\n",
    "    toks = [str(t).strip() for t in (dp_parts or []) if t is not None and str(t).strip() != \"\"]\n",
    "    return \"/\".join(toks) if toks else \"\"\n",
    "\n",
    "def dp_half_string(dp_text: str):\n",
    "    if dp_text is None or dp_text == \"\":\n",
    "        return \"\"\n",
    "    return f\"{dp_text}/2\"\n",
    "\n",
    "def canonical_exact(parts):\n",
    "    return \", \".join(parts)\n",
    "\n",
    "# ===================== Data Loading =====================\n",
    "def DataExtN_kernel():\n",
    "    \"\"\"\n",
    "    Build base DGL graph (NO self-loops) and return:\n",
    "      g (node attrs: bip, name, mol_idx, w; edge attrs: sigma, epsilon),\n",
    "      kernel_graphs (per unique monomer),\n",
    "      descs (per unique monomer, 5D),\n",
    "      mol_smiles (per unique monomer, EXACT same order as kernel_graphs)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    df = pd.read_excel('database_25Jun.xlsx', 'BCPs')\n",
    "    dfH = pd.read_excel('database_25Jun.xlsx', 'Homopolymers')\n",
    "\n",
    "    homopoly_bip = {}\n",
    "    for _, r in dfH.iterrows():\n",
    "        exact = str(r['Big_Smile']).strip()\n",
    "        try:\n",
    "            bip = int(r['Hard/Soft']) if not pd.isna(r['Hard/Soft']) else 0\n",
    "        except Exception:\n",
    "            bip = 0\n",
    "        homopoly_bip[exact] = bip\n",
    "\n",
    "    def split_blocks(big_smile_raw):\n",
    "        seq = clean_smiles(big_smile_raw)\n",
    "        if seq.startswith('{') and seq.endswith('}'):\n",
    "            seq = seq[1:-1]\n",
    "        return [clean_smiles(p) for p in seq.split('}{') if p.strip()]\n",
    "\n",
    "    def per_block_dp(dp_raw, k):\n",
    "        toks = [t.strip() for t in str(dp_raw).split(':')] if not pd.isna(dp_raw) else []\n",
    "        if len(toks) == k: return toks\n",
    "        if len(toks) == 1 and k >= 1: return toks * k\n",
    "        return (toks + [toks[-1] if toks else \"1\"] * k)[:k]\n",
    "\n",
    "    def clean_for_feats(mono: str) -> str:\n",
    "        return mono.replace(',', '').strip()\n",
    "\n",
    "    def dp_to_float(dp_txt: str) -> float:\n",
    "        try:\n",
    "            return float(dp_txt)\n",
    "        except Exception:\n",
    "            m = re.match(r'^\\s*([0-9]+(?:\\.[0-9]+)?)', str(dp_txt))\n",
    "            return float(m.group(1)) if m else 1.0\n",
    "\n",
    "    unique_monomers = []\n",
    "    mon2idx = {}\n",
    "    def add_unique_monomer(smi_clean):\n",
    "        if smi_clean not in mon2idx:\n",
    "            mon2idx[smi_clean] = len(unique_monomers)\n",
    "            unique_monomers.append(smi_clean)\n",
    "        return mon2idx[smi_clean]\n",
    "\n",
    "    all_rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        blocks_exact = split_blocks(r['Big_Smile'])\n",
    "        blocks_clean = [clean_for_feats(b) for b in blocks_exact]\n",
    "        for bc in blocks_clean: add_unique_monomer(bc)\n",
    "        try:    sig = float(r['σbreak (MPa)'])\n",
    "        except: sig = 0.0\n",
    "        try:    eps = float(r['εbreak (%)'])\n",
    "        except: eps = 0.0\n",
    "        all_rows.append((blocks_exact, blocks_clean, str(r['Block DP']), sig, eps))\n",
    "\n",
    "    kernel_graphs, descs = [], []\n",
    "    for smi in unique_monomers:\n",
    "        gk = smiles_to_grakel_graph(smi)\n",
    "        kernel_graphs.append(gk)\n",
    "        descs.append(torch.tensor(get_descriptors(smi), dtype=torch.float))\n",
    "    descs = torch.stack(descs, dim=0)\n",
    "    mol_smiles = list(unique_monomers)\n",
    "\n",
    "    G_nx = nx.DiGraph()\n",
    "    pk_to_key = {}\n",
    "    name_counter = [0]\n",
    "\n",
    "    def ensure_node(monomer_exact, monomer_clean, dp_text_final):\n",
    "        pk = (monomer_clean, dp_text_final)\n",
    "        if pk in pk_to_key:\n",
    "            return pk_to_key[pk]\n",
    "        bip = homopoly_bip.get(monomer_exact, homopoly_bip.get(monomer_clean, 0))\n",
    "        mol_idx = mon2idx[monomer_clean]\n",
    "        # w = math.sqrt(max(dp_to_float(dp_text_final), 1e-9))\n",
    "        # w = max(dp_to_float(dp_text_final), 1e-9)\n",
    "        # If you prefer log weighting, replace with:\n",
    "        import math; w = math.log(max(dp_to_float(dp_text_final), math.e))\n",
    "        key = f\"{monomer_clean}|DP={dp_text_final}\"\n",
    "        G_nx.add_node(key, bip=int(bip), name=name_counter[0], mol_idx=int(mol_idx), w=float(w))\n",
    "        pk_to_key[pk] = key\n",
    "        name_counter[0] += 1\n",
    "        return key\n",
    "\n",
    "    for (blocks_exact, blocks_clean, dp_raw, sig, eps) in all_rows:\n",
    "        k = len(blocks_exact)\n",
    "        if k == 0: continue\n",
    "        dps = per_block_dp(dp_raw, k)\n",
    "        if k == 3 and (blocks_clean[0] == blocks_clean[2]):\n",
    "            A_exact_left, A_clean = blocks_exact[0], blocks_clean[0]\n",
    "            B_exact, B_clean = blocks_exact[1], blocks_clean[1]\n",
    "            dpA_text = str(dp_to_float(dps[0]) + dp_to_float(dps[2]))\n",
    "            dpB_text = dps[1]\n",
    "            src_key = ensure_node(A_exact_left, A_clean, dpA_text)\n",
    "            dst_key = ensure_node(B_exact, B_clean, dpB_text)\n",
    "            G_nx.add_edge(src_key, dst_key, sigma=torch.tensor([sig], dtype=torch.float),\n",
    "                          epsilon=torch.tensor([eps], dtype=torch.float))\n",
    "        else:\n",
    "            node_keys = []\n",
    "            for j in range(k):\n",
    "                exact_j, clean_j, dpj_text = blocks_exact[j], blocks_clean[j], dps[j]\n",
    "                node_keys.append(ensure_node(exact_j, clean_j, dpj_text))\n",
    "            for j in range(k - 1):\n",
    "                G_nx.add_edge(node_keys[j], node_keys[j + 1],\n",
    "                              sigma=torch.tensor([sig], dtype=torch.float),\n",
    "                              epsilon=torch.tensor([eps], dtype=torch.float))\n",
    "\n",
    "    g = dgl.from_networkx(\n",
    "        G_nx,\n",
    "        node_attrs=['bip', 'name', 'mol_idx', 'w'],\n",
    "        edge_attrs=['sigma', 'epsilon']\n",
    "    )\n",
    "    return g, kernel_graphs, descs, mol_smiles\n",
    "\n",
    "# ===================== ChemBERTa embeddings =====================\n",
    "def compute_chemberta_embeddings(\n",
    "    smiles,\n",
    "    model_name=CHEMBERTA_MODEL,\n",
    "    batch_size=CHEMBERTA_BATCH,\n",
    "    model_device=CHEMBERTA_DEVICE,\n",
    "    return_device=device\n",
    "):\n",
    "    if not USE_CHEMBERTA:\n",
    "        print(\"[INFO] USE_CHEMBERTA=False. Skipping pretrained embeddings.\")\n",
    "        return None\n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "    except Exception as e:\n",
    "        print(f\"[INFO] transformers not available ({e}). Skipping ChemBERTa.\")\n",
    "        return None\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name).to(model_device)\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"[INFO] Could not load ChemBERTa model '{model_name}': {e}. Continuing without.\")\n",
    "        return None\n",
    "\n",
    "    if len(smiles) == 0:\n",
    "        return None\n",
    "\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(smiles), batch_size):\n",
    "            batch = smiles[i:i+batch_size]\n",
    "            toks = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            toks = {k: v.to(model_device, non_blocking=True) for k, v in toks.items()}\n",
    "            out = model(**toks)\n",
    "            vec = out.last_hidden_state.mean(dim=1)  # [B, D]\n",
    "            embs.append(vec.detach())\n",
    "    if len(embs) == 0:\n",
    "        return None\n",
    "    embs = torch.cat(embs, dim=0)\n",
    "    embs = torch.nan_to_num(embs, nan=0.0, posinf=0.0, neginf=0.0).to(return_device, non_blocking=True)\n",
    "    print(f\"[INFO] ChemBERTa embeddings: {embs.shape} (computed on {model_device}, returned on {return_device})\")\n",
    "    return embs\n",
    "\n",
    "# ========================== Model ==========================\n",
    "class GATBlock(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, num_heads=4, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.gat = dgl.nn.GATConv(in_feats, out_feats // num_heads, num_heads,\n",
    "                                  feat_drop=p_drop, attn_drop=p_drop, residual=True)\n",
    "        self.ln  = nn.LayerNorm(out_feats)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "    def forward(self, g, x):\n",
    "        h = self.gat(g, x).flatten(1)\n",
    "        h = F.elu(h)\n",
    "        h = self.ln(h)\n",
    "        return self.drop(h)\n",
    "\n",
    "class GATEncoder(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_heads=4, p_drop=0.2, feat_drop_in=0.1):\n",
    "        super().__init__()\n",
    "        self.feat_drop_in = nn.Dropout(feat_drop_in)\n",
    "        self.layers = nn.ModuleList([\n",
    "            GATBlock(in_feats, h_feats, num_heads=num_heads, p_drop=p_drop),\n",
    "            GATBlock(h_feats,  h_feats, num_heads=num_heads, p_drop=p_drop),\n",
    "            GATBlock(h_feats,  h_feats, num_heads=num_heads, p_drop=p_drop),\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(h_feats, h_feats)\n",
    "    def forward(self, g, x):\n",
    "        h = self.feat_drop_in(x)\n",
    "        for block in self.layers:\n",
    "            h = block(g, h)\n",
    "        return self.out_proj(h)\n",
    "\n",
    "class BilinearLP(nn.Module):\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty(d, d))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            g.apply_edges(lambda e: {'lp_score': (e.src['h'] @ self.W * e.dst['h']).sum(1)})\n",
    "            return g.edata['lp_score']\n",
    "\n",
    "# ====================== Pairwise features ======================\n",
    "def pair_features_from(g, h, eidx=None):\n",
    "    u_all, v_all = g.edges()\n",
    "    if eidx is None:\n",
    "        uu, vv = u_all, v_all\n",
    "    else:\n",
    "        uu, vv = u_all[eidx], v_all[eidx]\n",
    "    hu = h[uu.long()]\n",
    "    hv = h[vv.long()]\n",
    "    had = hu * hv\n",
    "    hdiff = torch.abs(hu - hv)\n",
    "    x = torch.cat([hu, hv, had, hdiff], dim=1)\n",
    "    return x\n",
    "\n",
    "def pair_features_with_lp(g_pos, h, lp_head):\n",
    "    X = pair_features_from(g_pos, h)\n",
    "    with torch.no_grad():\n",
    "        lp_raw = lp_head(g_pos, h).view(-1, 1)\n",
    "        lp_sig = torch.sigmoid(lp_raw)\n",
    "    return torch.cat([X, lp_sig], dim=1)\n",
    "\n",
    "# ====================== WL kernel features ======================\n",
    "def build_fold_kernel_features(base_g, kernel_graphs, descs, train_idx, device):\n",
    "    u_all, v_all = base_g.edges()\n",
    "    u_tr = u_all[train_idx]; v_tr = v_all[train_idx]\n",
    "    mask = (u_tr != v_tr)\n",
    "    u_pos = u_tr[mask].tolist(); v_pos = v_tr[mask].tolist()\n",
    "\n",
    "    mol_idx_tensor = base_g.ndata['mol_idx'].long().cpu()\n",
    "    train_mols = sorted({int(mol_idx_tensor[int(n)].item()) for n in (u_pos + v_pos)})\n",
    "    if len(train_mols) == 0:\n",
    "        train_mols = list(range(len(kernel_graphs)))\n",
    "\n",
    "    train_graphs = [kernel_graphs[i] for i in train_mols]\n",
    "    all_graphs   = kernel_graphs\n",
    "\n",
    "    # Replace GraKeL with SimpleWLKernel\n",
    "    gk = SimpleWLKernel(n_iter=WL_HEIGHT, normalize=True)\n",
    "    _ = gk.fit_transform(train_graphs)\n",
    "    K_all = gk.transform(all_graphs)  # [num_mols, len(train_mols)]\n",
    "\n",
    "    K_all = torch.tensor(K_all, dtype=torch.float, device=device)\n",
    "    descs = descs.to(device)\n",
    "\n",
    "    w = base_g.ndata['w'].to(device).view(-1, 1)\n",
    "    node_mol_idx = base_g.ndata['mol_idx'].long().to(device)\n",
    "\n",
    "    K_nodes = K_all[node_mol_idx]\n",
    "    D_nodes = descs[node_mol_idx]\n",
    "    feat_kernel = (K_nodes + 1.0) * w\n",
    "    features = torch.cat([feat_kernel, D_nodes], dim=1)\n",
    "    return features, gk, train_mols\n",
    "\n",
    "# ====================== Negatives ======================\n",
    "def build_neg_pool_for_split(base_g, pos_graph_for_split):\n",
    "    eu_pos, ev_pos = pos_graph_for_split.edges()\n",
    "    existing = set(zip(eu_pos.tolist(), ev_pos.tolist()))\n",
    "    bip_nodes = (base_g.ndata['bip'] == 1).nonzero(as_tuple=False).view(-1).tolist()\n",
    "    neg_pool = [(i, j) for i in bip_nodes for j in bip_nodes if i != j and (i, j) not in existing]\n",
    "    return neg_pool\n",
    "\n",
    "def sample_hard_negatives(neg_pool, n_needed, full_h, epoch=None, max_epoch=None,\n",
    "                          base_mult=NEG_BASE_MULT, max_mult=NEG_MAX_MULT, device=device):\n",
    "    if epoch is not None and max_epoch is not None and max_epoch > 0:\n",
    "        hard = base_mult + int((max_mult - base_mult) * (epoch / max_epoch))\n",
    "    else:\n",
    "        hard = max_mult\n",
    "    m = min(len(neg_pool), max(n_needed * hard, n_needed))\n",
    "    candidates = random.sample(neg_pool, m) if m > 0 else []\n",
    "    sims = []\n",
    "    with torch.no_grad():\n",
    "        for uu_, vv_ in candidates:\n",
    "            sims.append(F.cosine_similarity(full_h[uu_], full_h[vv_], dim=0).item())\n",
    "    top = sorted(zip(candidates, sims), key=lambda x: x[1], reverse=True)[:n_needed]\n",
    "    if len(top) == 0:\n",
    "        return dgl.graph(([], []), num_nodes=full_h.shape[0], device=device)\n",
    "    nu = [p[0] for p, _ in top]; nv = [p[1] for p, _ in top]\n",
    "    sub = dgl.graph((nu, nv), num_nodes=full_h.shape[0], device=device)\n",
    "    return sub\n",
    "\n",
    "# ====================== Classifier & Loss ======================\n",
    "class EdgeClassifier(nn.Module):\n",
    "    \"\"\"Per-task logits for sigma-high and epsilon-high.\"\"\"\n",
    "    def __init__(self, in_dim, hidden=256, p_drop=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden, 2)  # [logit_sigma_hi, logit_epsilon_hi]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ====================== Labels & Metrics ======================\n",
    "def make_cls_labels_strict(g_pos, s_thr, e_thr):\n",
    "    s = g_pos.edata['sigma'].view(-1)\n",
    "    e = g_pos.edata['epsilon'].view(-1)\n",
    "    y_sigma = (s > s_thr).long()\n",
    "    y_eps   = (e > e_thr).long()\n",
    "    return torch.stack([y_sigma, y_eps], dim=1)\n",
    "\n",
    "def make_cls_labels_tolerant(g_pos, s_thr, s_tol, e_thr, e_tol):\n",
    "    s = g_pos.edata['sigma'].view(-1)\n",
    "    e = g_pos.edata['epsilon'].view(-1)\n",
    "    y_sigma = ((s > s_thr) | (torch.abs(s - s_thr) <= s_tol)).long()\n",
    "    y_eps   = ((e > e_thr) | (torch.abs(e - e_thr) <= e_tol)).long()\n",
    "    return torch.stack([y_sigma, y_eps], dim=1)\n",
    "\n",
    "def and_labels_tolerant(g_pos, s_thr, s_tol, e_thr, e_tol):\n",
    "    y = make_cls_labels_tolerant(g_pos, s_thr, s_tol, e_thr, e_tol)\n",
    "    return (y[:,0] & y[:,1]).long()\n",
    "\n",
    "def is_single_class(y):\n",
    "    y = np.asarray(y).ravel().astype(int)\n",
    "    return (np.unique(y).size < 2) or (y.size == 0)\n",
    "\n",
    "def safe_roc_auc(y_true, scores):\n",
    "    y = np.asarray(y_true).ravel().astype(int)\n",
    "    s = np.asarray(scores).ravel()\n",
    "    if is_single_class(y): return float('nan')\n",
    "    return float(roc_auc_score(y, s))\n",
    "\n",
    "def safe_average_precision(y_true, scores):\n",
    "    y = np.asarray(y_true).ravel().astype(int)\n",
    "    s = np.asarray(scores).ravel()\n",
    "    if (y.sum() == 0) or (y.size == 0): return float('nan')\n",
    "    return float(average_precision_score(y, s))\n",
    "\n",
    "def binary_metrics(y_true, y_pred_bin):\n",
    "    y = np.asarray(y_true).astype(int).ravel()\n",
    "    yhat = np.asarray(y_pred_bin).astype(int).ravel()\n",
    "    tp = int(((yhat == 1) & (y == 1)).sum())\n",
    "    tn = int(((yhat == 0) & (y == 0)).sum())\n",
    "    fp = int(((yhat == 1) & (y == 0)).sum())\n",
    "    fn = int(((yhat == 0) & (y == 1)).sum())\n",
    "\n",
    "    acc = (tp + tn) / max(len(y), 1)\n",
    "    tpr = tp / max(tp + fn, 1)\n",
    "    tnr = tn / max(tn + fp, 1)\n",
    "    bal_acc = 0.5 * (tpr + tnr)\n",
    "    prec = tp / max(tp + fp, 1)\n",
    "    rec  = tp / max(tp + fn, 1)\n",
    "    f1 = 0.0 if (prec + rec) == 0 else 2 * prec * rec / (prec + rec)\n",
    "    prev = y.mean() if len(y) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"n\": len(y), \"n_pos\": int(y.sum()), \"n_neg\": int(len(y)-y.sum()),\n",
    "        \"accuracy\": acc, \"balanced_acc\": bal_acc, \"precision\": prec,\n",
    "        \"recall\": rec, \"specificity\": tnr, \"f1\": f1, \"prevalence\": prev\n",
    "    }\n",
    "\n",
    "def search_best_thresholds_for_balacc(val_probs_s, val_probs_e, y_and):\n",
    "    ps = val_probs_s.detach().cpu().numpy().ravel()\n",
    "    pe = val_probs_e.detach().cpu().numpy().ravel()\n",
    "    y  = y_and.detach().cpu().numpy().ravel().astype(int)\n",
    "    grid = np.linspace(0.05, 0.95, 37)\n",
    "\n",
    "    best = (0.5, 0.5); best_score = -1.0\n",
    "    for tau_s in grid:\n",
    "        pred_s = (ps > tau_s)\n",
    "        for tau_e in grid:\n",
    "            yhat = (pred_s & (pe > tau_e)).astype(int)\n",
    "            tp = int(((yhat==1)&(y==1)).sum()); tn = int(((yhat==0)&(y==0)).sum())\n",
    "            fp = int(((yhat==1)&(y==0)).sum()); fn = int(((yhat==0)&(y==1)).sum())\n",
    "            tpr = tp / max(tp+fn,1); tnr = tn / max(tn+fp,1)\n",
    "            bal_acc = 0.5*(tpr+tnr)\n",
    "            if bal_acc > best_score:\n",
    "                best_score, best = bal_acc, (float(tau_s), float(tau_e))\n",
    "    return best\n",
    "\n",
    "# ======= Tolerance auto-tuning (optional) =======\n",
    "def candidate_tols_from_val(val_pos, s_thr, e_thr, q_list=(5,10,15,20,25,30,40,50,60,70,80)):\n",
    "    ts = val_pos.edata['sigma'].detach().cpu().numpy().ravel()\n",
    "    te = val_pos.edata['epsilon'].detach().cpu().numpy().ravel()\n",
    "    ds = np.abs(ts - s_thr); de = np.abs(te - e_thr)\n",
    "    s_cands = sorted({float(np.percentile(ds, q)) for q in q_list})\n",
    "    e_cands = sorted({float(np.percentile(de, q)) for q in q_list})\n",
    "    return s_cands, e_cands\n",
    "\n",
    "def tune_tols_by_kpi(val_pos, val_probs_s, val_probs_e, s_thr, e_thr):\n",
    "    s_grid, e_grid = candidate_tols_from_val(val_pos, s_thr, e_thr)\n",
    "    best = (0.0, 0.0); best_acc = -1.0\n",
    "    ps = val_probs_s; pe = val_probs_e\n",
    "    for s_tol in s_grid:\n",
    "        for e_tol in e_grid:\n",
    "            y_and_tol = and_labels_tolerant(val_pos, s_thr, s_tol, e_thr, e_tol).float().to(device)\n",
    "            tau_s, tau_e = search_best_thresholds_for_balacc(ps, pe, y_and_tol)\n",
    "            ps_np = ps.detach().cpu().numpy().ravel()\n",
    "            pe_np = pe.detach().cpu().numpy().ravel()\n",
    "            yhat_and = ((ps_np > tau_s) & (pe_np > tau_e)).astype(int)\n",
    "            y_true   = y_and_tol.cpu().numpy().ravel().astype(int)\n",
    "            acc = (yhat_and == y_true).mean() if y_true.size else 0.0\n",
    "            if acc > best_acc:\n",
    "                best_acc, best = acc, (float(s_tol), float(e_tol))\n",
    "    return best\n",
    "\n",
    "# ======== LP threshold helper ========\n",
    "def best_tau_balacc_from_posneg(p, n):\n",
    "    p = np.asarray(p).ravel()\n",
    "    n = np.asarray(n).ravel()\n",
    "    scores = np.concatenate([p, n])\n",
    "    labels = np.concatenate([np.ones_like(p, dtype=int), np.zeros_like(n)])\n",
    "    if scores.size == 0:\n",
    "        return 0.5, {\"accuracy\": float('nan'), \"balanced_acc\": float('nan')}\n",
    "    grid = np.linspace(0.05, 0.95, 37)\n",
    "    best_tau, best_bal = 0.5, -1.0\n",
    "    for tau in grid:\n",
    "        yhat = (scores > tau).astype(int)\n",
    "        mets = binary_metrics(labels, yhat)\n",
    "        if mets[\"balanced_acc\"] > best_bal:\n",
    "            best_bal = mets[\"balanced_acc\"]; best_tau = float(tau)\n",
    "    yhat = (scores > best_tau).astype(int)\n",
    "    mets = binary_metrics(labels, yhat)\n",
    "    return best_tau, mets\n",
    "\n",
    "# ====================== Two-stage per fold ======================\n",
    "def run_fold_two_stage(base_g, train_idx, val_idx, test_idx,\n",
    "                       lp_pretrain_epochs=LP_PRETRAIN_EPOCHS,\n",
    "                       seed=2, device=torch.device('cpu')):\n",
    "    set_seed(seed)\n",
    "    u, v = base_g.edges()\n",
    "\n",
    "    # ----- labelled subgraphs (NO self-loops) -----\n",
    "    def build_pos_sub(eidx):\n",
    "        uu, vv = u[eidx], v[eidx]\n",
    "        sub = dgl.graph((uu, vv), num_nodes=base_g.num_nodes(), device=device)\n",
    "        sub.ndata.update({k: base_g.ndata[k] for k in base_g.ndata})\n",
    "        sub.edata['sigma']   = base_g.edata['sigma'][eidx]\n",
    "        sub.edata['epsilon'] = base_g.edata['epsilon'][eidx]\n",
    "        return sub\n",
    "\n",
    "    train_pos = build_pos_sub(train_idx)\n",
    "    val_pos   = build_pos_sub(val_idx)\n",
    "    test_pos  = build_pos_sub(test_idx)\n",
    "\n",
    "    def degree_feats_from(g_pos):\n",
    "        indeg = g_pos.in_degrees().float().view(-1, 1)\n",
    "        outdeg = g_pos.out_degrees().float().view(-1, 1)\n",
    "        deg = torch.cat([indeg, outdeg], dim=1)\n",
    "        mu = deg.mean(0, keepdim=True); sd = deg.std(0, keepdim=True).clamp(min=1e-8)\n",
    "        return (deg - mu) / sd\n",
    "\n",
    "    deg_train = degree_feats_from(train_pos)\n",
    "    for G in (train_pos, val_pos, test_pos):\n",
    "        G.ndata['deg_feat'] = deg_train\n",
    "\n",
    "    # Encoder graphs WITH self-loops\n",
    "    train_enc = dgl.add_self_loop(train_pos)\n",
    "    val_enc   = dgl.add_self_loop(val_pos)\n",
    "    test_enc  = dgl.add_self_loop(test_pos)\n",
    "\n",
    "    # Neg pools\n",
    "    neg_pool_train = build_neg_pool_for_split(base_g, train_pos)\n",
    "    neg_pool_val   = build_neg_pool_for_split(base_g, val_pos)\n",
    "    neg_pool_test  = build_neg_pool_for_split(base_g, test_pos)\n",
    "\n",
    "    n_train_pos = len(train_idx)\n",
    "    n_val_pos   = len(val_idx)\n",
    "    n_test_pos  = len(test_idx)\n",
    "\n",
    "    # Models\n",
    "    in_feats = base_g.ndata['Feature'].shape[1] + 2  # +2 deg features\n",
    "    encoder = GATEncoder(in_feats, HIDDEN_DIM, num_heads=4, p_drop=0.2, feat_drop_in=0.1).to(device)\n",
    "    lp_head = BilinearLP(HIDDEN_DIM).to(device)\n",
    "\n",
    "    # =========================================================\n",
    "    # Stage 1: LP pretraining (encoder + bilinear LP)\n",
    "    # =========================================================\n",
    "    lp_params = list(encoder.parameters()) + list(lp_head.parameters())\n",
    "    opt_lp = torch.optim.Adam(lp_params, lr=1e-3, weight_decay=1e-4)\n",
    "    best_lp_state = None\n",
    "    best_val_lp_auc = -float('inf')\n",
    "    best_tau_lp = 0.5  # chosen on VAL\n",
    "\n",
    "    for epoch in range(lp_pretrain_epochs):\n",
    "        encoder.train(); lp_head.train()\n",
    "        x_train = torch.cat([train_enc.ndata['Feature'], train_enc.ndata['deg_feat']], dim=1)\n",
    "        h_train = encoder(train_enc, x_train)\n",
    "\n",
    "        # positives\n",
    "        pl_pos = lp_head(train_pos, h_train)\n",
    "\n",
    "        # negatives from TRAIN pool\n",
    "        with torch.no_grad(): full_h = h_train.detach()\n",
    "        train_neg = sample_hard_negatives(neg_pool_train, n_train_pos, full_h,\n",
    "                                          epoch=epoch, max_epoch=lp_pretrain_epochs,\n",
    "                                          base_mult=NEG_BASE_MULT, max_mult=NEG_MAX_MULT, device=device)\n",
    "        train_neg.ndata['Feature'] = train_pos.ndata['Feature']\n",
    "        train_neg.ndata['deg_feat'] = train_pos.ndata['deg_feat']\n",
    "        train_neg_enc = dgl.add_self_loop(train_neg)\n",
    "        x_neg = torch.cat([train_neg_enc.ndata['Feature'], train_neg_enc.ndata['deg_feat']], dim=1)\n",
    "        h_neg = encoder(train_neg_enc, x_neg)\n",
    "        pl_neg = lp_head(train_neg, h_neg)\n",
    "\n",
    "        pos = pl_pos[:n_train_pos]; neg = pl_neg[:n_train_pos]\n",
    "        loss = F.margin_ranking_loss(pos, neg, torch.ones_like(pos), margin=0.2)\n",
    "        opt_lp.zero_grad(); loss.backward()\n",
    "        nn.utils.clip_grad_norm_(lp_params, max_norm=2.0); opt_lp.step()\n",
    "\n",
    "        # quick LP proxy on val\n",
    "        if (epoch + 1) % 50 == 0 or epoch == LP_PRETRAIN_EPOCHS - 1:\n",
    "            encoder.eval(); lp_head.eval()\n",
    "            with torch.no_grad():\n",
    "                x_val = torch.cat([val_enc.ndata['Feature'], val_enc.ndata['deg_feat']], dim=1)\n",
    "                h_val = encoder(val_enc, x_val)\n",
    "                vpos = lp_head(val_pos, h_val)\n",
    "\n",
    "                vneg_g = sample_hard_negatives(neg_pool_val, n_val_pos, h_val,\n",
    "                                               epoch=epoch, max_epoch=LP_PRETRAIN_EPOCHS,\n",
    "                                               base_mult=NEG_BASE_MULT, max_mult=NEG_MAX_MULT, device=device)\n",
    "                vneg_g.ndata['Feature'] = val_pos.ndata['Feature']\n",
    "                vneg_g.ndata['deg_feat'] = val_pos.ndata['deg_feat']\n",
    "                vneg_enc = dgl.add_self_loop(vneg_g)\n",
    "                xv = torch.cat([vneg_enc.ndata['Feature'], vneg_enc.ndata['deg_feat']], dim=1)\n",
    "                vneg = lp_head(vneg_g, encoder(vneg_enc, xv))\n",
    "\n",
    "                p = torch.sigmoid(vpos[:n_val_pos]).cpu().numpy().ravel()\n",
    "                n = torch.sigmoid(vneg[:n_val_pos]).cpu().numpy().ravel()\n",
    "                scores = np.concatenate([p, n]); labels = np.concatenate([np.ones_like(p), np.zeros_like(n)])\n",
    "                try:\n",
    "                    auc = roc_auc_score(labels, scores)\n",
    "                except Exception:\n",
    "                    auc = float('nan')\n",
    "\n",
    "                tau_lp, _ = best_tau_balacc_from_posneg(p, n)\n",
    "\n",
    "                if (not np.isnan(auc)) and (auc > best_val_lp_auc):\n",
    "                    best_val_lp_auc = float(auc)\n",
    "                    best_tau_lp = float(tau_lp)\n",
    "                    best_lp_state = (copy.deepcopy(encoder.state_dict()), copy.deepcopy(lp_head.state_dict()))\n",
    "\n",
    "    if best_lp_state is not None:\n",
    "        encoder.load_state_dict(best_lp_state[0]); lp_head.load_state_dict(best_lp_state[1])\n",
    "\n",
    "    # LP metrics on TEST\n",
    "    lp_metrics = {\"lp_auc_val\": float(best_val_lp_auc),\n",
    "                  \"lp_auc_test\": float('nan'),\n",
    "                  \"lp_acc_test\": float('nan'),\n",
    "                  \"lp_bal_acc_test\": float('nan'),\n",
    "                  \"tau_lp\": float(best_tau_lp)}\n",
    "    encoder.eval(); lp_head.eval()\n",
    "    with torch.no_grad():\n",
    "        x_te = torch.cat([test_enc.ndata['Feature'], test_enc.ndata['deg_feat']], dim=1)\n",
    "        h_te = encoder(test_enc, x_te)\n",
    "        tpos = lp_head(test_pos, h_te)\n",
    "\n",
    "        tneg_g = sample_hard_negatives(neg_pool_test, n_test_pos, h_te,\n",
    "                                       epoch=None, max_epoch=None,\n",
    "                                       base_mult=NEG_MAX_MULT, max_mult=NEG_MAX_MULT, device=device)\n",
    "        tneg_g.ndata['Feature'] = test_pos.ndata['Feature']\n",
    "        tneg_g.ndata['deg_feat'] = test_pos.ndata['deg_feat']\n",
    "        tneg_enc = dgl.add_self_loop(tneg_g)\n",
    "        xt = torch.cat([tneg_enc.ndata['Feature'], tneg_enc.ndata['deg_feat']], dim=1)\n",
    "        tneg = lp_head(tneg_g, encoder(tneg_enc, xt))\n",
    "\n",
    "        p = torch.sigmoid(tpos[:n_test_pos]).cpu().numpy().ravel()\n",
    "        n = torch.sigmoid(tneg[:n_test_pos]).cpu().numpy().ravel()\n",
    "\n",
    "        m = min(len(p), len(n))\n",
    "        p = p[:m]; n = n[:m]\n",
    "\n",
    "        if m > 0:\n",
    "            scores = np.concatenate([p, n]); labels = np.concatenate([np.ones_like(p), np.zeros_like(n)])\n",
    "            try:\n",
    "                lp_auc = roc_auc_score(labels, scores)\n",
    "            except Exception:\n",
    "                lp_auc = float('nan')\n",
    "            yhat = (scores > best_tau_lp).astype(int)\n",
    "            mets = binary_metrics(labels, yhat)\n",
    "            lp_metrics.update({\n",
    "                \"lp_auc_test\": float(lp_auc),\n",
    "                \"lp_acc_test\": float(mets[\"accuracy\"]),\n",
    "                \"lp_bal_acc_test\": float(mets[\"balanced_acc\"])\n",
    "            })\n",
    "\n",
    "    # =========================================================\n",
    "    # Stage 2: Classifier heads (σ/ε) on pair+LP features\n",
    "    # =========================================================\n",
    "    encoder.eval(); lp_head.eval()\n",
    "    with torch.no_grad():\n",
    "        x_tr_n = torch.cat([train_enc.ndata['Feature'], train_enc.ndata['deg_feat']], dim=1)\n",
    "        h_tr = encoder(train_enc, x_tr_n); X_tr = pair_features_with_lp(train_pos, h_tr, lp_head)\n",
    "        x_va_n = torch.cat([val_enc.ndata['Feature'], val_enc.ndata['deg_feat']], dim=1)\n",
    "        h_va = encoder(val_enc, x_va_n); X_va = pair_features_with_lp(val_pos, h_va, lp_head)\n",
    "        x_te_n = torch.cat([test_enc.ndata['Feature'], test_enc.ndata['deg_feat']], dim=1)\n",
    "        h_te = encoder(test_enc, x_te_n); X_te = pair_features_with_lp(test_pos, h_te, lp_head)\n",
    "\n",
    "    # Per-task labels for training (strict vs tolerant)\n",
    "    if TRAIN_WITH_TOLERANT_LABELS:\n",
    "        y_tr = make_cls_labels_tolerant(train_pos, SIGMA_THR, SIGMA_TOL, EPSILON_THR, EPSILON_TOL)\n",
    "        y_va = make_cls_labels_tolerant(val_pos,   SIGMA_THR, SIGMA_TOL, EPSILON_THR, EPSILON_TOL)\n",
    "        y_te = make_cls_labels_tolerant(test_pos,  SIGMA_THR, SIGMA_TOL, EPSILON_THR, EPSILON_TOL)\n",
    "    else:\n",
    "        y_tr = make_cls_labels_strict(train_pos, SIGMA_THR, EPSILON_THR)\n",
    "        y_va = make_cls_labels_strict(val_pos,   SIGMA_THR, EPSILON_THR)\n",
    "        y_te = make_cls_labels_strict(test_pos,  SIGMA_THR, EPSILON_THR)\n",
    "\n",
    "    clf = EdgeClassifier(X_tr.shape[1], hidden=256, p_drop=0.3).to(device)\n",
    "\n",
    "    # class imbalance handling\n",
    "    def pos_weight_from(y):\n",
    "        pw = []\n",
    "        for t in range(2):\n",
    "            pos = y[:, t].sum().item()\n",
    "            neg = y.shape[0] - pos\n",
    "            pw.append((neg / max(pos, 1)) if pos > 0 else 1.0)\n",
    "        return torch.tensor(pw, dtype=torch.float, device=device)\n",
    "\n",
    "    pos_weight = pos_weight_from(y_tr)\n",
    "    bce_per_task = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    opt = torch.optim.Adam(list(clf.parameters()), lr=2e-3, weight_decay=1e-4)\n",
    "\n",
    "    best_state = None\n",
    "    best_val_acc_tolAND = -1.0\n",
    "    best_sig_tol, best_eps_tol = float(SIGMA_TOL), float(EPSILON_TOL)\n",
    "    best_tau_s, best_tau_e = 0.5, 0.5\n",
    "    best_joint_lr = None\n",
    "    best_softAND_tau = 0.5 \n",
    "    best_calAND_tau = 0.5   \n",
    "\n",
    "    # Train classifier\n",
    "    for step in range(CLS_TRAIN_STEPS):\n",
    "        clf.train()\n",
    "        logits_2 = clf(X_tr)\n",
    "        loss_ce = bce_per_task(logits_2, y_tr.float())\n",
    "        opt.zero_grad(); loss_ce.backward()\n",
    "        nn.utils.clip_grad_norm_(list(clf.parameters()), 2.0)\n",
    "        opt.step()\n",
    "\n",
    "        if (step + 1) % 50 == 0 or step == CLS_TRAIN_STEPS - 1:\n",
    "            clf.eval()\n",
    "            with torch.no_grad():\n",
    "                val_logits_2 = clf(X_va)\n",
    "                val_probs_2 = torch.sigmoid(val_logits_2)\n",
    "\n",
    "                if AUTO_TUNE_TOLERANCES:\n",
    "                    s_tol_auto, e_tol_auto = tune_tols_by_kpi(\n",
    "                        val_pos, val_probs_2[:,0], val_probs_2[:,1],\n",
    "                        s_thr=SIGMA_THR, e_thr=EPSILON_THR\n",
    "                    )\n",
    "                else:\n",
    "                    s_tol_auto, e_tol_auto = float(SIGMA_TOL), float(EPSILON_TOL)\n",
    "\n",
    "                y_and_va_tol = and_labels_tolerant(val_pos, SIGMA_THR, s_tol_auto, EPSILON_THR, e_tol_auto).float().to(device)\n",
    "                tau_s, tau_e = search_best_thresholds_for_balacc(val_probs_2[:,0], val_probs_2[:,1], y_and_va_tol)\n",
    "\n",
    "                ps_np = val_probs_2[:,0].cpu().numpy().ravel()\n",
    "                pe_np = val_probs_2[:,1].cpu().numpy().ravel()\n",
    "                y_true   = y_and_va_tol.cpu().numpy().ravel().astype(int)\n",
    "\n",
    "                # Fit calibrated joint combiner on VAL (if labels not degenerate)\n",
    "                joint_lr = None\n",
    "                best_tau_cal_curr = 0.5\n",
    "                if np.unique(y_true).size > 1:\n",
    "                    X_val_joint = np.column_stack([\n",
    "                        ps_np,\n",
    "                        pe_np,\n",
    "                        ps_np * pe_np,\n",
    "                        np.minimum(ps_np, pe_np)\n",
    "                    ])\n",
    "                    try:\n",
    "                        jlr = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "                        jlr.fit(X_val_joint, y_true)\n",
    "                        # tune calibrated τ on VAL by balanced accuracy\n",
    "                        pcal_val = jlr.predict_proba(X_val_joint)[:,1]\n",
    "                        grid = np.linspace(0.05, 0.95, 37)\n",
    "                        best_bal_cal = -1.0\n",
    "                        for tau in grid:\n",
    "                            yhat = (pcal_val >= tau).astype(int)\n",
    "                            mets = binary_metrics(y_true, yhat)\n",
    "                            if mets[\"balanced_acc\"] > best_bal_cal:\n",
    "                                best_bal_cal = mets[\"balanced_acc\"]\n",
    "                                best_tau_cal_curr = float(tau)\n",
    "                        joint_lr = jlr\n",
    "                    except Exception:\n",
    "                        joint_lr = None\n",
    "\n",
    "                # --- choose soft-AND(min) τ on VAL by balanced accuracy ---\n",
    "                score_val_min = np.minimum(ps_np, pe_np)\n",
    "                grid = np.linspace(0.05, 0.95, 37)\n",
    "                best_tau_min, best_bal_min = 0.5, -1.0\n",
    "                for tau in grid:\n",
    "                    yhat = (score_val_min >= tau).astype(int)\n",
    "                    mets = binary_metrics(y_true, yhat)\n",
    "                    if mets[\"balanced_acc\"] > best_bal_min:\n",
    "                        best_bal_min = mets[\"balanced_acc\"]\n",
    "                        best_tau_min = float(tau)\n",
    "\n",
    "                # selection criterion: AND Acc on VAL with these τs (kept as in prior code)\n",
    "                yhat_and = ((ps_np > tau_s) & (pe_np > tau_e)).astype(int)\n",
    "                mets_and = binary_metrics(y_true, yhat_and)\n",
    "                val_acc_tolAND = mets_and[\"accuracy\"]\n",
    "\n",
    "                if val_acc_tolAND > best_val_acc_tolAND:\n",
    "                    best_val_acc_tolAND = val_acc_tolAND\n",
    "                    best_sig_tol, best_eps_tol = float(s_tol_auto), float(e_tol_auto)\n",
    "                    best_tau_s, best_tau_e = float(tau_s), float(tau_e)\n",
    "                    best_softAND_tau = best_tau_min\n",
    "                    best_calAND_tau = best_tau_cal_curr\n",
    "                    best_state = copy.deepcopy(clf.state_dict())\n",
    "                    best_joint_lr = copy.deepcopy(joint_lr) if joint_lr is not None else None\n",
    "\n",
    "    if best_state is not None:\n",
    "        clf.load_state_dict(best_state)\n",
    "\n",
    "    # ======= Test (reporting only) =======\n",
    "    clf.eval()\n",
    "    with torch.no_grad():\n",
    "        te_logits_2 = clf(X_te)\n",
    "        te_probs_2  = torch.sigmoid(te_logits_2)\n",
    "        p_s, p_e = te_probs_2[:,0], te_probs_2[:,1]\n",
    "\n",
    "        # Tolerant AND labels on TEST with the selected tolerances\n",
    "        y_and_te_tol = and_labels_tolerant(test_pos, SIGMA_THR, best_sig_tol, EPSILON_THR, best_eps_tol)\n",
    "        y_and_true = y_and_te_tol.cpu().numpy().ravel().astype(int)\n",
    "\n",
    "        # Per-task tolerant labels\n",
    "        y_te_tol = make_cls_labels_tolerant(test_pos, SIGMA_THR, best_sig_tol, EPSILON_THR, best_eps_tol).cpu().numpy()\n",
    "        y_sigma = y_te_tol[:,0].astype(int).ravel()\n",
    "        y_eps   = y_te_tol[:,1].astype(int).ravel()\n",
    "        p_s_np  = p_s.cpu().numpy().ravel()\n",
    "        p_e_np  = p_e.cpu().numpy().ravel()\n",
    "\n",
    "        # Per-task AUROC/AUPRC (threshold-free)\n",
    "        auc_sigma = safe_roc_auc(y_sigma, p_s_np)\n",
    "        auc_eps   = safe_roc_auc(y_eps,   p_e_np)\n",
    "        ap_sigma  = safe_average_precision(y_sigma, p_s_np)\n",
    "        ap_eps    = safe_average_precision(y_eps,   p_e_np)\n",
    "        auc_macro = np.nanmean([auc_sigma, auc_eps])\n",
    "        ap_macro  = np.nanmean([ap_sigma,  ap_eps])\n",
    "\n",
    "        # Per-task thresholded metrics using VAL-tuned τσ*, τε*\n",
    "        yhat_sigma = (p_s_np > best_tau_s).astype(int)\n",
    "        yhat_eps   = (p_e_np > best_tau_e).astype(int)\n",
    "\n",
    "        mets_sigma = binary_metrics(y_sigma, yhat_sigma)\n",
    "        mets_eps   = binary_metrics(y_eps,   yhat_eps)\n",
    "\n",
    "        acc_sigma = mets_sigma[\"accuracy\"];      bal_sigma = mets_sigma[\"balanced_acc\"]\n",
    "        acc_eps   = mets_eps[\"accuracy\"];        bal_eps   = mets_eps[\"balanced_acc\"]\n",
    "\n",
    "        # derive specificity = TNR = 2*BalAcc - TPR(=recall)\n",
    "        spec_sigma = mets_sigma[\"balanced_acc\"]*2 - mets_sigma[\"recall\"]\n",
    "        spec_eps   = mets_eps[\"balanced_acc\"]*2   - mets_eps[\"recall\"]\n",
    "\n",
    "        prec_sigma, rec_sigma, f1_sigma = (\n",
    "            mets_sigma[\"precision\"], mets_sigma[\"recall\"], mets_sigma[\"f1\"]\n",
    "        )\n",
    "        prec_eps, rec_eps, f1_eps = (\n",
    "            mets_eps[\"precision\"], mets_eps[\"recall\"], mets_eps[\"f1\"]\n",
    "        )\n",
    "\n",
    "        # Macro per-task (at VAL-tuned per-task thresholds)\n",
    "        acc_macro  = np.nanmean([acc_sigma,  acc_eps])\n",
    "        bal_macro  = np.nanmean([bal_sigma,  bal_eps])\n",
    "        prec_macro = np.nanmean([prec_sigma, prec_eps])\n",
    "        rec_macro  = np.nanmean([rec_sigma,  rec_eps])\n",
    "        spec_macro = np.nanmean([spec_sigma, spec_eps])\n",
    "        # f1_macro   = np.nanmean([f1_sigma,   f1_eps])\n",
    "        f1_macro   = (2 * prec_macro * rec_macro) / (prec_macro + rec_macro) if (prec_macro + rec_macro) > 0 else np.nan\n",
    "\n",
    "        # Hard AND (normal) decision (reference)\n",
    "        yhat_and = (yhat_sigma & yhat_eps).astype(int)\n",
    "        met_and  = binary_metrics(y_and_true, yhat_and)\n",
    "        and_spec = met_and[\"balanced_acc\"]*2 - met_and[\"recall\"]  # derive specificity (TNR)\n",
    "\n",
    "        # --- Soft AND composites (overall) ---\n",
    "        score_and_min   = np.minimum(p_s_np, p_e_np)\n",
    "        score_and_prod  = p_s_np * p_e_np\n",
    "        score_and_luka  = np.maximum(0.0, p_s_np + p_e_np - 1.0)\n",
    "        score_and_geom  = np.sqrt(np.maximum(1e-12, p_s_np * p_e_np))\n",
    "\n",
    "        auc_and_min  = safe_roc_auc(y_and_true, score_and_min)\n",
    "        ap_and_min   = safe_average_precision(y_and_true, score_and_min)\n",
    "        auc_and_prod = safe_roc_auc(y_and_true, score_and_prod)\n",
    "        ap_and_prod  = safe_average_precision(y_and_true, score_and_prod)\n",
    "        auc_and_luka = safe_roc_auc(y_and_true, score_and_luka)\n",
    "        ap_and_luka  = safe_average_precision(y_and_true, score_and_luka)\n",
    "        auc_and_geom = safe_roc_auc(y_and_true, score_and_geom)\n",
    "        ap_and_geom  = safe_average_precision(y_and_true, score_and_geom)\n",
    "\n",
    "        # Thresholded soft-AND(min) at VAL-tuned τ\n",
    "        yhat_softAND = (score_and_min >= best_softAND_tau).astype(int)\n",
    "        met_softAND  = binary_metrics(y_and_true, yhat_softAND)\n",
    "        soft_prec = float(met_softAND[\"precision\"])\n",
    "        soft_rec  = float(met_softAND[\"recall\"])\n",
    "        soft_spec = float(met_softAND[\"balanced_acc\"] * 2 - met_softAND[\"recall\"])\n",
    "\n",
    "        # --- Calibrated joint combiner (VAL-trained logistic) ---\n",
    "        if best_joint_lr is not None:\n",
    "            X_test_joint = np.column_stack([p_s_np, p_e_np, p_s_np*p_e_np, np.minimum(p_s_np, p_e_np)])\n",
    "            try:\n",
    "                p_and_cal = best_joint_lr.predict_proba(X_test_joint)[:,1]\n",
    "                auc_and_cal = safe_roc_auc(y_and_true, p_and_cal)\n",
    "                ap_and_cal  = safe_average_precision(y_and_true, p_and_cal)\n",
    "            except Exception:\n",
    "                p_and_cal = None\n",
    "                auc_and_cal, ap_and_cal = float('nan'), float('nan')\n",
    "        else:\n",
    "            p_and_cal = None\n",
    "            auc_and_cal, ap_and_cal = float('nan'), float('nan')\n",
    "\n",
    "        # Thresholded calibrated-AND at τ* (fallback to 0.5 if not set upstream)\n",
    "        if p_and_cal is not None:\n",
    "            try:\n",
    "                cal_tau = float(best_calAND_tau)\n",
    "            except NameError:\n",
    "                cal_tau = 0.5\n",
    "            yhat_calAND = (p_and_cal >= cal_tau).astype(int)\n",
    "            met_calAND  = binary_metrics(y_and_true, yhat_calAND)\n",
    "            cal_prec = float(met_calAND[\"precision\"])\n",
    "            cal_rec = float(met_calAND[\"recall\"])\n",
    "            cal_spec = float(met_calAND[\"balanced_acc\"]*2 - met_calAND[\"recall\"])\n",
    "        else:\n",
    "            cal_tau = float('nan')\n",
    "            met_calAND = {\"accuracy\": float('nan'), \"balanced_acc\": float('nan'),\n",
    "                          \"precision\": float('nan'), \"recall\": float('nan'), \"f1\": float('nan')}\n",
    "            cal_prec = cal_rec = cal_spec = float('nan')\n",
    "\n",
    "        # Confident-subset metrics for calibrated AND (gentler bands 0.60/0.40)\n",
    "        if p_and_cal is not None:\n",
    "            conf_mask = (p_and_cal >= CONF_HI) | (p_and_cal <= CONF_LO)\n",
    "            conf_cov = float(conf_mask.mean())\n",
    "            if conf_mask.any():\n",
    "                y_conf = y_and_true[conf_mask]\n",
    "                yhat_conf = (p_and_cal[conf_mask] > 0.5).astype(int)\n",
    "                mets_conf = binary_metrics(y_conf, yhat_conf)\n",
    "                conf_acc = float(mets_conf[\"accuracy\"])\n",
    "                conf_bal = float(mets_conf[\"balanced_acc\"])\n",
    "            else:\n",
    "                conf_acc = float('nan')\n",
    "                conf_bal = float('nan')\n",
    "        else:\n",
    "            conf_cov, conf_acc, conf_bal = float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    metrics = {\n",
    "        # Selected tolerances & thresholds\n",
    "        'sigma_tol': best_sig_tol, 'epsilon_tol': best_eps_tol,\n",
    "        'tau_sigma': best_tau_s, 'tau_epsilon': best_tau_e,\n",
    "\n",
    "        # Per-task diagnostics vs tolerant labels (threshold-free)\n",
    "        'auc_sigma': auc_sigma, 'auc_epsilon': auc_eps,\n",
    "        'ap_sigma': ap_sigma, 'ap_epsilon': ap_eps,\n",
    "        'auc_macro': auc_macro, 'ap_macro': ap_macro,\n",
    "\n",
    "        # Per-task thresholded (VAL-tuned τσ*, τε*)\n",
    "        'sigma_accuracy': float(acc_sigma), 'epsilon_accuracy': float(acc_eps),\n",
    "        'sigma_balanced_acc': float(bal_sigma), 'epsilon_balanced_acc': float(bal_eps),\n",
    "        'sigma_precision': float(prec_sigma), 'sigma_recall': float(rec_sigma),\n",
    "        'sigma_specificity': float(spec_sigma), 'sigma_f1': float(f1_sigma),\n",
    "        'epsilon_precision': float(prec_eps), 'epsilon_recall': float(rec_eps),\n",
    "        'epsilon_specificity': float(spec_eps), 'epsilon_f1': float(f1_eps),\n",
    "        'accuracy_macro': float(acc_macro), 'balanced_acc_macro': float(bal_macro),\n",
    "        'precision_macro': float(prec_macro), 'recall_macro': float(rec_macro),\n",
    "        'specificity_macro': float(spec_macro), 'f1_macro': float(f1_macro),\n",
    "\n",
    "        # Hard-AND (normal) vs tolerant labels\n",
    "        'and_acc_tol': met_and[\"accuracy\"],\n",
    "        'and_bal_acc_tol': met_and[\"balanced_acc\"],\n",
    "        'and_precision_tol': float(met_and[\"precision\"]),\n",
    "        'and_recall_tol': float(met_and[\"recall\"]),\n",
    "        'and_specificity_tol': float(and_spec),\n",
    "        'and_f1_tol': met_and[\"f1\"],\n",
    "        'and_prev_tol': met_and[\"prevalence\"],\n",
    "        'and_n': met_and[\"n\"], 'and_n_pos': met_and[\"n_pos\"], 'and_n_neg': met_and[\"n_neg\"],\n",
    "\n",
    "        # Confident-subset stats for calibrated AND\n",
    "        'cal_conf_cov': conf_cov, 'cal_conf_acc': conf_acc, 'cal_conf_bal_acc': conf_bal,\n",
    "\n",
    "        # LP metrics\n",
    "        'lp_auc_val': lp_metrics[\"lp_auc_val\"],\n",
    "        'lp_auc_test': lp_metrics[\"lp_auc_test\"],\n",
    "        'lp_acc_test': lp_metrics[\"lp_acc_test\"],\n",
    "        'lp_bal_acc_test': lp_metrics[\"lp_bal_acc_test\"],\n",
    "        'tau_lp': lp_metrics[\"tau_lp\"],\n",
    "    }\n",
    "\n",
    "    # return models for inference\n",
    "    return metrics, encoder, lp_head, clf, best_joint_lr\n",
    "\n",
    "# ====================== K-Fold driver (returns artifacts) ======================\n",
    "\n",
    "def train_evaluate_kfold_and_collect(k=5, seed=2):\n",
    "    \"\"\"\n",
    "    Tidy K-fold training/eval with compact, paper-ready metrics.\n",
    "\n",
    "    Prints:\n",
    "      • Per-fold concise metrics\n",
    "      • Sectioned Summary (mean ± std across folds)\n",
    "      • OOF (micro-averaged) metrics\n",
    "\n",
    "    Saves:\n",
    "      • per_fold_metrics.csv\n",
    "      • summary_metrics.csv\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "\n",
    "    # ---------- Build base graph + features ----------\n",
    "    g, kernel_graphs, descs, mol_smiles = DataExtN_kernel()\n",
    "    g = g.to(device)\n",
    "\n",
    "    chemberta = compute_chemberta_embeddings(\n",
    "        mol_smiles,\n",
    "        model_name=CHEMBERTA_MODEL,\n",
    "        batch_size=CHEMBERTA_BATCH,\n",
    "        model_device=CHEMBERTA_DEVICE,\n",
    "        return_device=device\n",
    "    )\n",
    "    used_chemberta = chemberta is not None\n",
    "    emb_dim = int(chemberta.shape[1]) if used_chemberta else 0\n",
    "\n",
    "    eids = np.arange(g.num_edges())\n",
    "    u_all, v_all = g.edges()\n",
    "\n",
    "    def and_labels_tolerant_on_full_graph(G, s_thr, s_tol, e_thr, e_tol):\n",
    "        s = G.edata['sigma'].view(-1)\n",
    "        e = G.edata['epsilon'].view(-1)\n",
    "        y_sigma = ((s > s_thr) | (torch.abs(s - s_thr) <= s_tol)).long()\n",
    "        y_eps   = ((e > e_thr) | (torch.abs(e - e_thr) <= e_tol)).long()\n",
    "        return (y_sigma & y_eps).cpu().numpy().astype(int).ravel()\n",
    "\n",
    "    y_all_and_tol = and_labels_tolerant_on_full_graph(g, SIGMA_THR, SIGMA_TOL, EPSILON_THR, EPSILON_TOL)\n",
    "\n",
    "    # Choose KFold vs StratifiedKFold safely\n",
    "    use_strat_kfold = False\n",
    "    if np.unique(y_all_and_tol).size > 1:\n",
    "        _, counts = np.unique(y_all_and_tol, return_counts=True)\n",
    "        if counts.min() >= k:\n",
    "            use_strat_kfold = True\n",
    "\n",
    "    if use_strat_kfold:\n",
    "        kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        split_iter = kf.split(eids, y_all_and_tol)\n",
    "    else:\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "        split_iter = kf.split(eids)\n",
    "\n",
    "    # ---------- OOF accumulators ----------\n",
    "    def cat(lst): return np.concatenate(lst) if len(lst) else np.array([])\n",
    "    oof_p_s, oof_p_e = [], []\n",
    "    oof_y_sigma, oof_y_epsilon, oof_y_and = [], [], []\n",
    "    oof_score_min, oof_score_prod, oof_score_luka, oof_score_geom = [], [], [], []\n",
    "    oof_score_cal = []\n",
    "\n",
    "    per_fold_rows = []\n",
    "\n",
    "    all_metrics = []  \n",
    "    fold_artifacts = []\n",
    "\n",
    "    # ---------- CV loop ----------\n",
    "    for fold_id, (train_all, test_idx) in enumerate(split_iter, start=1):\n",
    "        def can_stratify(y, test_size):\n",
    "            if y is None: return False\n",
    "            vals, counts = np.unique(y, return_counts=True)\n",
    "            if len(vals) < 2: return False\n",
    "            need = int(np.ceil(max(1.0/test_size, 1.0/(1.0 - test_size))))\n",
    "            return counts.min() >= need\n",
    "\n",
    "        strat = y_all_and_tol[train_all] if np.unique(y_all_and_tol).size > 1 else None\n",
    "        TEST_SIZE = 0.2\n",
    "        if can_stratify(strat, TEST_SIZE):\n",
    "            train_idx, val_idx = train_test_split(train_all, test_size=TEST_SIZE, random_state=seed, stratify=strat)\n",
    "        else:\n",
    "            train_idx, val_idx = train_test_split(train_all, test_size=TEST_SIZE, random_state=seed, stratify=None)\n",
    "\n",
    "        fold_features, gk, train_mols = build_fold_kernel_features(g, kernel_graphs, descs, train_idx, device)\n",
    "        if used_chemberta:\n",
    "            node_mol_idx = g.ndata['mol_idx'].long().to(device)\n",
    "            emb_nodes = chemberta[node_mol_idx]\n",
    "            fold_features = torch.cat([fold_features, emb_nodes], dim=1)\n",
    "\n",
    "        train_nodes = torch.unique(torch.cat([u_all[train_idx], v_all[train_idx]])).long()\n",
    "        f_mu = fold_features[train_nodes].mean(dim=0, keepdim=True)\n",
    "        f_sd = fold_features[train_nodes].std(dim=0, keepdim=True).clamp(min=1e-8)\n",
    "        norm_features = (fold_features - f_mu) / f_sd\n",
    "        g.ndata['Feature'] = norm_features\n",
    "\n",
    "        # Train this fold\n",
    "        m, encoder, lp_head, clf, joint_lr = run_fold_two_stage(\n",
    "            g, train_idx, val_idx, test_idx,\n",
    "            lp_pretrain_epochs=LP_PRETRAIN_EPOCHS,\n",
    "            seed=seed, device=device\n",
    "        )\n",
    "        all_metrics.append(m)\n",
    "\n",
    "        fold_artifacts.append({\n",
    "            \"encoder\": encoder,\n",
    "            \"lp_head\": lp_head,\n",
    "            \"clf\": clf,\n",
    "            \"joint_lr\": joint_lr,\n",
    "            \"sigma_tol\": float(m['sigma_tol']),\n",
    "            \"epsilon_tol\": float(m['epsilon_tol']),\n",
    "            \"tau_sigma\": float(m['tau_sigma']),\n",
    "            \"tau_epsilon\": float(m['tau_epsilon']),\n",
    "            \"train_mu\": f_mu.detach().clone(),\n",
    "            \"train_sd\": f_sd.detach().clone(),\n",
    "            \"gk\": gk,\n",
    "            \"train_mols_graphs\": [kernel_graphs[i] for i in train_mols],\n",
    "            \"used_chemberta\": used_chemberta,\n",
    "            \"emb_dim\": emb_dim,\n",
    "        })\n",
    "\n",
    "        def fmt(x):\n",
    "            return \"N/A\" if (x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x)))) else f\"{x:.4f}\"\n",
    "\n",
    "        print(f\"\\n=== Fold {fold_id}/{k} ===\")\n",
    "        print(\"----- Threshold-free -----\")\n",
    "        print(f\"Macro AUROC/AP: {fmt(m['auc_macro'])}/{fmt(m['ap_macro'])}\")\n",
    "        print(\"----- Thresholded (macro @ τ*) -----\")\n",
    "        print(f\"Precision: {fmt(m['precision_macro'])} | Recall: {fmt(m['recall_macro'])} | \"\n",
    "              f\"F1: {fmt(m['f1_macro'])} | Specificity: {fmt(m['specificity_macro'])}\")\n",
    "        print(\"----- Link Prediction -----\")\n",
    "        print(f\"LP AUROC (val/test): {fmt(m['lp_auc_val'])}/{fmt(m['lp_auc_test'])}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            def degree_feats_from(g_pos):\n",
    "                indeg = g_pos.in_degrees().float().view(-1, 1)\n",
    "                outdeg = g_pos.out_degrees().float().view(-1, 1)\n",
    "                deg = torch.cat([indeg, outdeg], dim=1)\n",
    "                mu = deg.mean(0, keepdim=True); sd = deg.std(0, keepdim=True).clamp(min=1e-8)\n",
    "                return (deg - mu) / sd\n",
    "\n",
    "            train_pos = dgl.graph((u_all[train_idx], v_all[train_idx]), num_nodes=g.num_nodes(), device=device)\n",
    "            deg_train = degree_feats_from(train_pos)\n",
    "\n",
    "            test_pos = dgl.graph((u_all[test_idx], v_all[test_idx]), num_nodes=g.num_nodes(), device=device)\n",
    "            test_pos.edata['sigma']   = g.edata['sigma'][test_idx]\n",
    "            test_pos.edata['epsilon'] = g.edata['epsilon'][test_idx]\n",
    "            test_pos.ndata['Feature'] = g.ndata['Feature']\n",
    "            test_pos.ndata['deg_feat'] = deg_train\n",
    "\n",
    "            test_enc = dgl.add_self_loop(test_pos)\n",
    "            x_te = torch.cat([test_enc.ndata['Feature'], test_enc.ndata['deg_feat']], dim=1)\n",
    "            h_test = encoder(test_enc, x_te)\n",
    "            te_probs = torch.sigmoid(clf(pair_features_with_lp(test_pos, h_test, lp_head)))\n",
    "            p_s = te_probs[:,0].cpu().numpy().ravel()\n",
    "            p_e = te_probs[:,1].cpu().numpy().ravel()\n",
    "\n",
    "            y_te_tol = make_cls_labels_tolerant(test_pos, SIGMA_THR, m['sigma_tol'], EPSILON_THR, m['epsilon_tol']).cpu().numpy()\n",
    "            y_sigma = y_te_tol[:,0].astype(int).ravel()\n",
    "            y_eps   = y_te_tol[:,1].astype(int).ravel()\n",
    "            y_and   = (y_sigma & y_eps).astype(int)\n",
    "\n",
    "            score_min  = np.minimum(p_s, p_e)\n",
    "            score_prod = p_s * p_e\n",
    "            score_luka = np.maximum(0.0, p_s + p_e - 1.0)\n",
    "            score_geom = np.sqrt(np.maximum(1e-12, p_s * p_e))\n",
    "\n",
    "            if joint_lr is not None:\n",
    "                X_joint = np.column_stack([p_s, p_e, p_s*p_e, np.minimum(p_s, p_e)])\n",
    "                try:\n",
    "                    score_cal = joint_lr.predict_proba(X_joint)[:,1]\n",
    "                except Exception:\n",
    "                    score_cal = None\n",
    "            else:\n",
    "                score_cal = None\n",
    "\n",
    "        # Keep OOF arrays for downstream use (no micro summary printing)\n",
    "        oof_p_s.append(p_s); oof_p_e.append(p_e)\n",
    "        oof_y_sigma.append(y_sigma); oof_y_epsilon.append(y_eps); oof_y_and.append(y_and)\n",
    "        oof_score_min.append(score_min); oof_score_prod.append(score_prod)\n",
    "        oof_score_luka.append(score_luka); oof_score_geom.append(score_geom)\n",
    "        if score_cal is not None: oof_score_cal.append(score_cal)\n",
    "\n",
    "        # Row for CSV per-fold table — keep only requested metrics\n",
    "        per_fold_rows.append({\n",
    "            \"fold\": fold_id,\n",
    "            \"auc_macro\": m['auc_macro'],\n",
    "            \"ap_macro\": m['ap_macro'],\n",
    "            \"precision_macro\": m['precision_macro'],\n",
    "            \"recall_macro\": m['recall_macro'],\n",
    "            \"specificity_macro\": m['specificity_macro'],\n",
    "            \"f1_macro\": m['f1_macro'],\n",
    "            \"lp_auc_val\": m['lp_auc_val'],\n",
    "            \"lp_auc_test\": m['lp_auc_test'],\n",
    "        })\n",
    "\n",
    "\n",
    "    # ---------- Summary (mean ± std across folds) ----------\n",
    "    def nanmean_std(arr):\n",
    "        arr = np.array(arr, dtype=float)\n",
    "        m = np.nanmean(arr) if arr.size else float('nan')\n",
    "        s = float(np.nanstd(arr, ddof=1)) if np.isfinite(arr).sum() > 1 else 0.0\n",
    "        return m, s\n",
    "    def fmtMS(t, nd=4):\n",
    "        m, s = t\n",
    "        fm = \"N/A\" if (np.isnan(m) or np.isinf(m)) else f\"{m:.{nd}f}\"\n",
    "        fs = \"N/A\" if (np.isnan(s) or np.isinf(s)) else f\"{s:.{nd}f}\"\n",
    "        return fm, fs\n",
    "\n",
    "    df = pd.DataFrame(per_fold_rows)\n",
    "    def ms(col): return fmtMS(nanmean_std(df[col].values)) if col in df else (\"N/A\",\"N/A\")\n",
    "\n",
    "    print(\"\\n===== Summary (means ± std across folds) =====\")\n",
    "\n",
    "    print(\"----- Threshold-free (macro) -----\")\n",
    "    for label, col in [\n",
    "        (\"Macro AUROC\", \"auc_macro\"),\n",
    "        (\"Macro AUPRC\", \"ap_macro\"),\n",
    "    ]:\n",
    "        fm, fs = ms(col); print(f\"{label}: {fm} ± {fs}\")\n",
    "\n",
    "    print(\"----- Thresholded (macro @ τ*) -----\")\n",
    "    for label, col in [\n",
    "        (\"Precision (macro)\", \"precision_macro\"),\n",
    "        (\"Recall (macro)\", \"recall_macro\"),\n",
    "        (\"Specificity (macro)\", \"specificity_macro\"),\n",
    "        (\"F1 (macro)\", \"f1_macro\"),\n",
    "    ]:\n",
    "        fm, fs = ms(col); print(f\"{label}: {fm} ± {fs}\")\n",
    "\n",
    "    # Section 3: Link Prediction (LP)\n",
    "    print(\"----- Link Prediction -----\")\n",
    "    for label, col in [\n",
    "        (\"LP AUROC (val)\", \"lp_auc_val\"),\n",
    "        (\"LP AUROC (test)\", \"lp_auc_test\"),\n",
    "    ]:\n",
    "        fm, fs = ms(col); print(f\"{label}: {fm} ± {fs}\")\n",
    "\n",
    "    print(\"*******************************************************************************************\")\n",
    "\n",
    "    # ---------- OOF data (kept for return bundle; no micro summary printing) ----------\n",
    "    Pσ, Pε = cat(oof_p_s), cat(oof_p_e)\n",
    "    Yσ, Yε = cat(oof_y_sigma), cat(oof_y_epsilon)\n",
    "    Yand   = cat(oof_y_and)\n",
    "    Smin, Sprod, Sluka, Sgeom = cat(oof_score_min), cat(oof_score_prod), cat(oof_score_luka), cat(oof_score_geom)\n",
    "    Scal   = cat(oof_score_cal) if len(oof_score_cal) else None\n",
    "\n",
    "    # ---------- Write CSVs ----------\n",
    "    per_fold_df = pd.DataFrame(per_fold_rows)\n",
    "    per_fold_df.to_csv(\"per_fold_metrics.csv\", index=False)\n",
    "\n",
    "    _keep_cols = [\n",
    "        \"fold\",\n",
    "        \"precision_macro\", \"recall_macro\", \"f1_macro\", \"specificity_macro\",\n",
    "        \"ap_macro\", \"auc_macro\",\n",
    "        \"lp_auc_val\", \"lp_auc_test\",\n",
    "    ]\n",
    "    _per_fold_trim = per_fold_df.loc[:, [c for c in _keep_cols if c in per_fold_df.columns]]\n",
    "    _per_fold_trim.to_csv(\"per_fold_metrics_trimmed.csv\", index=False)\n",
    "\n",
    "\n",
    "    summary_rows = []\n",
    "    def add_rows(section, pairs):\n",
    "        for label, col in pairs:\n",
    "            m, s = nanmean_std(per_fold_df[col].values if col in per_fold_df else [])\n",
    "            summary_rows.append({\"section\": section, \"metric\": label, \"mean\": m, \"std\": s})\n",
    "\n",
    "    # Section 1: Threshold-free macro\n",
    "    add_rows(\"Threshold-free (macro)\", [\n",
    "        (\"Macro AUROC\", \"auc_macro\"),\n",
    "        (\"Macro AUPRC\", \"ap_macro\"),\n",
    "    ])\n",
    "\n",
    "    # Section 2: Thresholded macro @ τ*\n",
    "    add_rows(\"Thresholded (macro @ τ*)\", [\n",
    "        (\"Precision (macro)\", \"precision_macro\"),\n",
    "        (\"Recall (macro)\", \"recall_macro\"),\n",
    "        (\"Specificity (macro)\", \"specificity_macro\"),\n",
    "        (\"F1 (macro)\", \"f1_macro\"),\n",
    "    ])\n",
    "\n",
    "    # Section 3: Link Prediction\n",
    "    add_rows(\"Link Prediction\", [\n",
    "        (\"LP AUROC (val)\", \"lp_auc_val\"),\n",
    "        (\"LP AUROC (test)\", \"lp_auc_test\"),\n",
    "    ])\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    summary_df.to_csv(\"summary_metrics.csv\", index=False)\n",
    "    \n",
    "    _keep_metrics = {\n",
    "        \"Macro AUROC\",\n",
    "        \"Macro AUPRC\",\n",
    "        \"Precision (macro)\",\n",
    "        \"Recall (macro)\",\n",
    "        \"Specificity (macro)\",\n",
    "        \"F1 (macro)\",\n",
    "        \"LP AUROC (val)\",\n",
    "        \"LP AUROC (test)\",\n",
    "    }\n",
    "    _summary_trim = summary_df[summary_df[\"metric\"].isin(_keep_metrics)].copy()\n",
    "    _summary_trim.to_csv(\"summary_metrics_trimmed.csv\", index=False)\n",
    "\n",
    "\n",
    "    def _thr_str(x):\n",
    "        try:\n",
    "            xf = float(x)\n",
    "            return str(int(xf)) if xf.is_integer() else str(xf).replace(\".\", \"p\")\n",
    "        except Exception:\n",
    "            return str(x)\n",
    "\n",
    "    sig_s = _thr_str(SIGMA_THR)\n",
    "    eps_s = _thr_str(EPSILON_THR)\n",
    "    excel_path = f\"ResultsNew/stress_strain_summary_sigma{sig_s}_epsilon{eps_s}.xlsx\"\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelWriter(excel_path, engine=\"xlsxwriter\") as writer:\n",
    "            # Sheet: Summary (means ± std)\n",
    "            summary_df.to_excel(writer, index=False, sheet_name=\"stress_and_strain\")\n",
    "            ws1 = writer.sheets[\"stress_and_strain\"]\n",
    "            r1, c1 = summary_df.shape\n",
    "            ws1.add_table(0, 0, r1, c1 - 1, {\n",
    "                \"name\": \"StressAndStrain\",\n",
    "                \"columns\": [{\"header\": col} for col in summary_df.columns]\n",
    "            })\n",
    "            for i, col in enumerate(summary_df.columns):\n",
    "                width = max(12, int(summary_df[col].astype(str).map(len).max()) + 2)\n",
    "                ws1.set_column(i, i, width)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Could not create named Excel tables (falling back to plain Excel): {e}\")\n",
    "        with pd.ExcelWriter(excel_path) as writer:\n",
    "            summary_df.to_excel(writer, index=False, sheet_name=\"stress_and_strain\")\n",
    "\n",
    "    print(f\"Saved: per_fold_metrics.csv, summary_metrics.csv, {excel_path} \"\n",
    "          f\"(table: StressAndStrain)\")\n",
    "\n",
    "    # --- Pack OOF for downstream threshold sweeps ---\n",
    "    oof_bundle = {\n",
    "        \"p_sigma\": Pσ,          # OOF probabilities for σ-high\n",
    "        \"p_epsilon\": Pε,        # OOF probabilities for ε-high\n",
    "        \"y_sigma\": Yσ.astype(int),\n",
    "        \"y_epsilon\": Yε.astype(int),\n",
    "        \"y_and\": Yand.astype(int),\n",
    "        \"score_min\": Smin,      # min(pσ, pε)\n",
    "        \"score_prod\": Sprod,    # pσ * pε\n",
    "    }\n",
    "    if Scal is not None and Scal.size == Yand.size:\n",
    "        oof_bundle[\"score_cal\"] = Scal\n",
    "\n",
    "    return all_metrics, fold_artifacts, oof_bundle\n",
    "\n",
    "\n",
    "\n",
    "# ================== Inference feature builder ==================\n",
    "# NOTE: You asked not to remove this function. The body is kept intact.\n",
    "# It continues to call `gk.transform([g])` which is fully compatible with SimpleWLKernel.\n",
    "# (We kept the function name `smiles_to_grakel_graph` to avoid edits inside.)\n",
    "\n",
    "def make_node_feature_for_smiles_parts(parts, dp_parts_text, gk, train_mols_graphs,\n",
    "                                       use_chemberta=True, chemberta_cache=None, device=device):\n",
    "\n",
    "    K_dim = len(train_mols_graphs)\n",
    "    K_sum = torch.zeros(K_dim, dtype=torch.float, device=device)\n",
    "    D_sum = torch.zeros(5, dtype=torch.float, device=device)\n",
    "    EMB_sum = None\n",
    "\n",
    "    def try_float(x):\n",
    "        try: return float(x)\n",
    "        except: return None\n",
    "\n",
    "    graphs = []\n",
    "    for p in parts:\n",
    "        g = smiles_to_grakel_graph(p)\n",
    "        graphs.append(g)\n",
    "\n",
    "    for i, p in enumerate(parts):\n",
    "        dp_txt = dp_parts_text[i] if i < len(dp_parts_text) else None\n",
    "        dp_val = try_float(dp_txt) if dp_txt is not None else None\n",
    "        w = math.sqrt(dp_val) if (dp_val is not None and dp_val > 0) else 1.0\n",
    "\n",
    "        g = graphs[i]\n",
    "        if g is not None:\n",
    "            Ki = gk.transform([g])  # [1, K_dim]\n",
    "            Ki = torch.tensor(Ki[0], dtype=torch.float, device=device)\n",
    "            K_sum += (Ki + 1.0) * w\n",
    "\n",
    "        D_sum += torch.tensor(get_descriptors(p), dtype=torch.float, device=device)\n",
    "\n",
    "        if use_chemberta and chemberta_cache is not None:\n",
    "            if p not in chemberta_cache:\n",
    "                emb_one = compute_chemberta_embeddings(\n",
    "                    [p],\n",
    "                    model_name=CHEMBERTA_MODEL,\n",
    "                    batch_size=CHEMBERTA_BATCH,\n",
    "                    model_device=CHEMBERTA_DEVICE,\n",
    "                    return_device=device\n",
    "                )\n",
    "                chemberta_cache[p] = emb_one if emb_one is not None else None\n",
    "            if chemberta_cache[p] is not None:\n",
    "                vec = chemberta_cache[p][0].to(device)\n",
    "                EMB_sum = vec if EMB_sum is None else (EMB_sum + vec)\n",
    "\n",
    "    feat_list = [K_sum, D_sum]\n",
    "    if use_chemberta and EMB_sum is not None:\n",
    "        feat_list.append(EMB_sum)\n",
    "    feat_vec = torch.cat(feat_list, dim=0)\n",
    "    return feat_vec\n",
    "\n",
    "def normalize_features_with_train_stats(feat_vec, train_mu, train_sd):\n",
    "    return (feat_vec - train_mu) / train_sd.clamp(min=1e-8)\n",
    "def metrics_from_scores(y_true, scores, tau):\n",
    "    y = np.asarray(y_true).astype(int).ravel()\n",
    "    s = np.asarray(scores).ravel()\n",
    "    yhat = (s >= tau).astype(int)\n",
    "    return binary_metrics(y, yhat)\n",
    "def sweep_hard_and_grid(oof, taus=(0.3, 0.5, 0.7), out_csv=\"paper_hardAND_grid.csv\"):\n",
    "\n",
    "    pσ, pε, y_and = oof[\"p_sigma\"], oof[\"p_epsilon\"], oof[\"y_and\"]\n",
    "    rows = []\n",
    "    for tau_s in taus:\n",
    "        for tau_e in taus:\n",
    "            yhat = ((pσ >= tau_s) & (pε >= tau_e)).astype(int)\n",
    "            mets = binary_metrics(y_and, yhat)\n",
    "            rows.append({\n",
    "                \"tau_sigma\": tau_s, \"tau_epsilon\": tau_e,\n",
    "                \"n\": mets[\"n\"], \"prevalence\": mets[\"prevalence\"],\n",
    "                \"accuracy\": mets[\"accuracy\"], \"balanced_acc\": mets[\"balanced_acc\"],\n",
    "                \"precision\": mets[\"precision\"], \"recall\": mets[\"recall\"],\n",
    "                \"specificity\": max(0.0, min(1.0, 2*mets[\"balanced_acc\"] - mets[\"recall\"])),  # TNR\n",
    "                \"f1\": mets[\"f1\"],\n",
    "            })\n",
    "    df = pd.DataFrame(rows).sort_values(\n",
    "        by=[\"balanced_acc\",\"precision\",\"recall\"], ascending=False\n",
    "    )\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[WROTE] {out_csv}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def sweep_soft_and_min(oof, taus=np.round(np.arange(0.3, 0.81, 0.05), 2),\n",
    "                       out_csv=\"paper_softANDmin_sweep.csv\"):\n",
    "    \"\"\"\n",
    "    Decision is 1{ min(pσ, pε) >= tau } on OOF.\n",
    "    \"\"\"\n",
    "    y_and, smin = oof[\"y_and\"], oof[\"score_min\"]\n",
    "    rows = []\n",
    "    for tau in taus:\n",
    "        mets = metrics_from_scores(y_and, smin, tau)\n",
    "        rows.append({\n",
    "            \"tau_soft_min\": tau,\n",
    "            \"n\": mets[\"n\"], \"prevalence\": mets[\"prevalence\"],\n",
    "            \"accuracy\": mets[\"accuracy\"], \"balanced_acc\": mets[\"balanced_acc\"],\n",
    "            \"precision\": mets[\"precision\"], \"recall\": mets[\"recall\"],\n",
    "            \"specificity\": max(0.0, min(1.0, 2*mets[\"balanced_acc\"] - mets[\"recall\"])),\n",
    "            \"f1\": mets[\"f1\"],\n",
    "        })\n",
    "    df = pd.DataFrame(rows).sort_values(by=[\"balanced_acc\",\"precision\",\"recall\"], ascending=False)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[WROTE] {out_csv}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def sweep_calibrated_and(oof, taus=np.round(np.arange(0.3, 0.81, 0.05), 2),\n",
    "                         out_csv=\"paper_calibratedAND_sweep.csv\"):\n",
    "    \"\"\"\n",
    "    Decision is 1{ p_and_cal >= tau }. Skips if calibrated scores are absent.\n",
    "    \"\"\"\n",
    "    if \"score_cal\" not in oof:\n",
    "        print(\"[INFO] No calibrated AND scores in OOF — skipping.\")\n",
    "        return pd.DataFrame()\n",
    "    y_and, scal = oof[\"y_and\"], oof[\"score_cal\"]\n",
    "    rows = []\n",
    "    for tau in taus:\n",
    "        mets = metrics_from_scores(y_and, scal, tau)\n",
    "        rows.append({\n",
    "            \"tau_cal\": tau,\n",
    "            \"n\": mets[\"n\"], \"prevalence\": mets[\"prevalence\"],\n",
    "            \"accuracy\": mets[\"accuracy\"], \"balanced_acc\": mets[\"balanced_acc\"],\n",
    "            \"precision\": mets[\"precision\"], \"recall\": mets[\"recall\"],\n",
    "            \"specificity\": max(0.0, min(1.0, 2*mets[\"balanced_acc\"] - mets[\"recall\"])),\n",
    "            \"f1\": mets[\"f1\"],\n",
    "        })\n",
    "    df = pd.DataFrame(rows).sort_values(by=[\"balanced_acc\",\"precision\",\"recall\"], ascending=False)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[WROTE] {out_csv}\")\n",
    "    return df\n",
    "def lp_filter_tradeoff_on_oof(oof, lp_scores, lp_taus=(0.3, 0.4, 0.5, 0.6),\n",
    "                              base_scores=\"score_min\",  # or \"score_cal\"\n",
    "                              base_tau=0.5,\n",
    "                              out_csv=\"paper_lp_filter_tradeoff.csv\"):\n",
    "    \"\"\"\n",
    "    Keep examples with lp_scores >= tau_lp, then apply AND decision by base_scores >= base_tau.\n",
    "    lp_scores: array-like OOF link probabilities for the *same examples in oof*.\n",
    "    \"\"\"\n",
    "    y_and = oof[\"y_and\"]\n",
    "    s = oof[base_scores] if isinstance(base_scores, str) else base_scores\n",
    "    rows = []\n",
    "    lp = np.asarray(lp_scores).ravel()\n",
    "    for tau_lp in lp_taus:\n",
    "        keep = (lp >= tau_lp)\n",
    "        cov = float(keep.mean())\n",
    "        if keep.any():\n",
    "            mets = metrics_from_scores(y_and[keep], s[keep], base_tau)\n",
    "            acc = mets[\"accuracy\"]; bal = mets[\"balanced_acc\"]\n",
    "            prec, rec = mets[\"precision\"], mets[\"recall\"]\n",
    "            f1 = mets[\"f1\"]\n",
    "        else:\n",
    "            acc=bal=prec=rec=f1=np.nan\n",
    "        rows.append({\n",
    "            \"tau_lp\": tau_lp, \"coverage\": cov,\n",
    "            \"accuracy\": acc, \"balanced_acc\": bal,\n",
    "            \"precision\": prec, \"recall\": rec, \"f1\": f1\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"[WROTE] {out_csv}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfe952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChemBERTa embeddings: torch.Size([39, 768]) (computed on cpu, returned on cpu)\n",
      "\n",
      "=== Fold 1/4 ===\n",
      "----- Threshold-free -----\n",
      "Macro AUROC/AP: 0.8172/0.8126\n",
      "----- Thresholded (macro @ τ*) -----\n",
      "Precision: 0.6962 | Recall: 0.6714 | F1: 0.6836 | Specificity: 0.7738\n",
      "----- Link Prediction -----\n",
      "LP AUROC (val/test): 1.0000/1.0000\n",
      "\n",
      "=== Fold 2/4 ===\n",
      "----- Threshold-free -----\n",
      "Macro AUROC/AP: 0.8012/0.8153\n",
      "----- Thresholded (macro @ τ*) -----\n",
      "Precision: 0.7000 | Recall: 0.7067 | F1: 0.7033 | Specificity: 0.6500\n",
      "----- Link Prediction -----\n",
      "LP AUROC (val/test): 1.0000/1.0000\n"
     ]
    }
   ],
   "source": [
    "all_metrics, fold_artifacts, oof = train_evaluate_kfold_and_collect(k=4, seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbc64bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ChemBERTa embeddings: torch.Size([88, 768]) (computed on cpu, returned on cpu)\n",
     ]
    }
   ],
   "source": [
    "# ===================== New-node inference & export (WEIGHTED logit-mean) =====================\n",
    "\n",
    "def predict_links_for_homopolymers(\n",
    "    fold_artifacts,\n",
    "    fold_weights=None,          \n",
    "    MIN_SUPPORT=0,\n",
    "    pair_batch=200000,         \n",
    "    *,\n",
    "    CANONICALIZE_FOR_DEDUP=False,\n",
    "    STRICT_DEDUP=True,\n",
    "    LP_FILTER=None              \n",
    "):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # ---- helpers ----\n",
    "    def _sigmoid(x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def _logit_clip(p, eps=1e-6):\n",
    "        p = np.asarray(p, dtype=float)\n",
    "        p = np.clip(p, eps, 1 - eps)\n",
    "        return np.log(p / (1.0 - p))\n",
    "\n",
    "    def _weighted_median(vals, weights):\n",
    "        vals = np.asarray(vals, dtype=float)\n",
    "        weights = np.asarray(weights, dtype=float)\n",
    "        m = np.isfinite(vals) & np.isfinite(weights) & (weights >= 0)\n",
    "        vals, weights = vals[m], weights[m]\n",
    "        if vals.size == 0 or weights.sum() <= 0:\n",
    "            return float('nan')\n",
    "        order = np.argsort(vals)\n",
    "        v = vals[order]; w = weights[order]\n",
    "        cw = np.cumsum(w)\n",
    "        cutoff = 0.5 * w.sum()\n",
    "        i = int(np.searchsorted(cw, cutoff, side=\"left\"))\n",
    "        i = min(i, len(v) - 1)\n",
    "        return float(v[i])\n",
    "\n",
    "    # ---- load Homopolymers ----\n",
    "    dfH = pd.read_excel('database_25Jun.xlsx', 'Homopolymers')\n",
    "\n",
    "    def _split_exact_to_parts(exact_str: str):\n",
    "        s = str(exact_str).strip()\n",
    "        if s == \"\" or s.lower() == \"nan\":\n",
    "            return []\n",
    "        parts = [p.strip() for p in s.split(',') if p.strip() != \"\"]\n",
    "        return parts if parts else [s]\n",
    "\n",
    "    raw_records = []\n",
    "    for idx, row in dfH.iterrows():\n",
    "        exact_raw = str(row.get('Big_Smile', '')).strip()\n",
    "        parts = _split_exact_to_parts(exact_raw)\n",
    "        canon = canonical_exact(parts) if CANONICALIZE_FOR_DEDUP else exact_raw\n",
    "\n",
    "        s = str(row.get('Overall DP', '')).strip()\n",
    "        if s == '' or s.lower() == 'nan':\n",
    "            dps = [None] * len(parts)\n",
    "        else:\n",
    "            toks = [t.strip() for t in s.split('/') if t.strip() != \"\"]\n",
    "            if len(toks) == 0:\n",
    "                dps = [None] * len(parts)\n",
    "            elif len(toks) == 1 and len(parts) >= 1:\n",
    "                dps = [toks[0]] * len(parts)\n",
    "            elif len(toks) == len(parts):\n",
    "                dps = toks\n",
    "            else:\n",
    "                dps = (toks + [toks[-1]] * len(parts))[:len(parts)]\n",
    "\n",
    "        raw_records.append({\n",
    "            \"rid\": int(idx),\n",
    "            \"exact\": exact_raw,\n",
    "            \"canon_exact\": canon,\n",
    "            \"parts\": parts,\n",
    "            \"dp_parts\": dps,\n",
    "            \"dp_join\": joined_dp_text(dps)\n",
    "        })\n",
    "\n",
    "    # ---- Deduplicate ----\n",
    "    if STRICT_DEDUP:\n",
    "        unique_map = {}\n",
    "        for rec in raw_records:\n",
    "            key = (rec[\"canon_exact\"], rec[\"dp_join\"])\n",
    "            if key not in unique_map:\n",
    "                unique_map[key] = rec\n",
    "        records = list(unique_map.values())\n",
    "    else:\n",
    "        records = raw_records\n",
    "\n",
    "    records = sorted(records, key=lambda r: (r[\"canon_exact\"], r[\"dp_join\"], r[\"rid\"]))\n",
    "\n",
    "    if len(records) == 0 or len(fold_artifacts) == 0:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"pair_string\",\"dp_string\",\"support_folds\",\n",
    "            \"p_sigma\",\"p_epsilon\",\"p_lp\",\"tau_sigma\",\"tau_epsilon\"\n",
    "        ])\n",
    "\n",
    "    # ---- fold weights ----\n",
    "    F = len(fold_artifacts)\n",
    "    if fold_weights is None:\n",
    "        fold_weights = np.ones(F, dtype=float)\n",
    "    else:\n",
    "        fold_weights = np.asarray(fold_weights, dtype=float)\n",
    "        assert fold_weights.shape[0] == F, \"fold_weights must match number of folds\"\n",
    "        # normalize to mean 1 (keeps scale similar to unweighted case)\n",
    "        s = fold_weights.mean()\n",
    "        fold_weights = (fold_weights / s) if s > 0 else np.ones(F, dtype=float)\n",
    "\n",
    "    # ---- ChemBERTa precompute if any fold used it ----\n",
    "    any_cb = any(b.get('used_chemberta', False) for b in fold_artifacts)\n",
    "    if any_cb:\n",
    "        all_unique_parts = sorted({p for r in records for p in r[\"parts\"]})\n",
    "        chemberta_map = {}\n",
    "        if len(all_unique_parts) > 0:\n",
    "            embs = compute_chemberta_embeddings(\n",
    "                all_unique_parts,\n",
    "                model_name=CHEMBERTA_MODEL,\n",
    "                batch_size=CHEMBERTA_BATCH,\n",
    "                model_device=CHEMBERTA_DEVICE,\n",
    "                return_device=device\n",
    "            )\n",
    "            if embs is not None and embs.shape[0] == len(all_unique_parts):\n",
    "                for i, smi in enumerate(all_unique_parts):\n",
    "                    chemberta_map[smi] = embs[i:i+1]\n",
    "            else:\n",
    "                chemberta_map = None\n",
    "        else:\n",
    "            chemberta_map = None\n",
    "    else:\n",
    "        chemberta_map = None\n",
    "\n",
    "    # ---- Per-fold node features ----\n",
    "    row_ids = [rec[\"rid\"] for rec in records]\n",
    "    rid_to_pos = {rid: i for i, rid in enumerate(row_ids)}\n",
    "    pos_to_rid = {i: rid for i, rid in enumerate(row_ids)}\n",
    "\n",
    "    per_fold_node_feat = []\n",
    "    for bundle in fold_artifacts:\n",
    "        gk = bundle['gk']\n",
    "        train_mols_graphs = bundle['train_mols_graphs']\n",
    "        f_mu = bundle['train_mu'].to(device)\n",
    "        f_sd = bundle['train_sd'].to(device)\n",
    "        use_cb = (chemberta_map is not None) and bundle.get('used_chemberta', False)\n",
    "\n",
    "        row_feat = {}\n",
    "        for rec in records:\n",
    "            feat_vec = make_node_feature_for_smiles_parts(\n",
    "                rec[\"parts\"], rec[\"dp_parts\"], gk, train_mols_graphs,\n",
    "                use_chemberta=use_cb, chemberta_cache=chemberta_map, device=device\n",
    "            )\n",
    "            feat_vec = normalize_features_with_train_stats(feat_vec.unsqueeze(0), f_mu, f_sd).squeeze(0)\n",
    "            row_feat[rec[\"rid\"]] = {\"feat\": feat_vec, \"dp_join\": rec[\"dp_join\"], \"exact\": rec[\"exact\"]}\n",
    "        per_fold_node_feat.append(row_feat)\n",
    "\n",
    "    # ---- Ordered pairs (A != B) ----\n",
    "    pairs_pos = [(i, j) for i in range(len(row_ids)) for j in range(len(row_ids)) if i != j]\n",
    "    if len(pairs_pos) == 0:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"pair_string\",\"dp_string\",\"support_folds\",\n",
    "            \"p_sigma\",\"p_epsilon\",\"p_lp\",\"tau_sigma\",\"tau_epsilon\"\n",
    "        ])\n",
    "\n",
    "    exacts  = {rec[\"rid\"]: rec[\"exact\"]   for rec in records}\n",
    "    dp_join = {rec[\"rid\"]: rec[\"dp_join\"] for rec in records}\n",
    "    dp_half = {rid: (dp_half_string(dp_join[rid]) if dp_join[rid] else \"\") for rid in row_ids}\n",
    "\n",
    "    # ---- Weighted accumulators (LOGIT sums and weight sums per pair) ----\n",
    "    M = len(pairs_pos)\n",
    "    sumL_p_sigma = np.zeros(M, dtype=np.float64)\n",
    "    sumL_p_eps   = np.zeros(M, dtype=np.float64)\n",
    "    sumL_p_lp    = np.zeros(M, dtype=np.float64)\n",
    "\n",
    "    sumW_sigma = np.zeros(M, dtype=np.float64)\n",
    "    sumW_eps   = np.zeros(M, dtype=np.float64)\n",
    "    sumW_lp    = np.zeros(M, dtype=np.float64)\n",
    "\n",
    "    support = np.zeros(M, dtype=np.int32)  # unweighted count of AND-positive votes\n",
    "\n",
    "    # Keep τ's per fold (global, not per pair) for weighted medians\n",
    "    folds_tau_sigma   = []\n",
    "    folds_tau_epsilon = []\n",
    "\n",
    "    src_pos_arr = np.array([s for (s, t) in pairs_pos], dtype=np.int64)\n",
    "    tgt_pos_arr = np.array([t for (s, t) in pairs_pos], dtype=np.int64)\n",
    "\n",
    "    # ---- Score across folds ----\n",
    "    for f_idx, (bundle, row_feat) in enumerate(zip(fold_artifacts, per_fold_node_feat)):\n",
    "        w = float(fold_weights[f_idx])\n",
    "\n",
    "        encoder: GATEncoder = bundle['encoder']\n",
    "        lp_head: BilinearLP = bundle['lp_head']\n",
    "        clf: EdgeClassifier  = bundle['clf']\n",
    "        tau_s = float(bundle['tau_sigma'])\n",
    "        tau_e = float(bundle['tau_epsilon'])\n",
    "\n",
    "        folds_tau_sigma.append(tau_s)\n",
    "        folds_tau_epsilon.append(tau_e)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            F = torch.stack([row_feat[rid][\"feat\"] for rid in row_ids], dim=0)  # [N, Din]\n",
    "            deg_zeros = torch.zeros(F.shape[0], 2, device=device)\n",
    "            X_in = torch.cat([F, deg_zeros], dim=1)\n",
    "\n",
    "            G = dgl.graph(([], []), num_nodes=F.shape[0], device=device)\n",
    "            G = dgl.add_self_loop(G)\n",
    "\n",
    "            encoder.eval(); lp_head.eval(); clf.eval()\n",
    "            H = encoder(G, X_in)      # [N, D]\n",
    "            W = lp_head.W             # [D, D]\n",
    "\n",
    "            for start in range(0, M, pair_batch):\n",
    "                end = min(start + pair_batch, M)\n",
    "                idx = slice(start, end)\n",
    "\n",
    "                hs = H[src_pos_arr[idx]]  # [B, D]\n",
    "                ht = H[tgt_pos_arr[idx]]  # [B, D]\n",
    "\n",
    "                lp_raw = (hs @ W) * ht\n",
    "                lp_raw = lp_raw.sum(dim=1, keepdim=True)\n",
    "                lp_sig = torch.sigmoid(lp_raw)                      # [B,1]\n",
    "                lp_prob = lp_sig.detach().cpu().numpy().ravel()     # [B]\n",
    "\n",
    "                X_pair = torch.cat([hs, ht, hs * ht, torch.abs(hs - ht), lp_sig], dim=1)\n",
    "                logits = clf(X_pair)\n",
    "                probs  = torch.sigmoid(logits)\n",
    "                p_s = probs[:, 0].detach().cpu().numpy()\n",
    "                p_e = probs[:, 1].detach().cpu().numpy()\n",
    "\n",
    "                Ls = _logit_clip(p_s); Le = _logit_clip(p_e); Ll = _logit_clip(lp_prob)\n",
    "                sumL_p_sigma[idx] += w * Ls;   sumW_sigma[idx] += w\n",
    "                sumL_p_eps[idx]   += w * Le;   sumW_eps[idx]   += w\n",
    "                sumL_p_lp[idx]    += w * Ll;   sumW_lp[idx]    += w\n",
    "\n",
    "                yhat_and = ((p_s > tau_s) & (p_e > tau_e)).astype(np.int32)\n",
    "                support[idx] += yhat_and\n",
    "\n",
    "    # ---- Weighted median thresholds across folds ----\n",
    "    tau_sigma_med   = _weighted_median(folds_tau_sigma,   fold_weights) if len(folds_tau_sigma)   else 0.5\n",
    "    tau_epsilon_med = _weighted_median(folds_tau_epsilon, fold_weights) if len(folds_tau_epsilon) else 0.5\n",
    "    if not np.isfinite(tau_sigma_med):   tau_sigma_med = 0.5\n",
    "    if not np.isfinite(tau_epsilon_med): tau_epsilon_med = 0.5\n",
    "\n",
    "    # ---- Build dataframe ----\n",
    "    out_rows = []\n",
    "    for i, (s_pos, t_pos) in enumerate(pairs_pos):\n",
    "        rid_s = pos_to_rid[s_pos]\n",
    "        rid_t = pos_to_rid[t_pos]\n",
    "        if (MIN_SUPPORT is None) or (support[i] >= int(MIN_SUPPORT)):\n",
    "            src_exact = exacts[rid_s]\n",
    "            tgt_exact = exacts[rid_t]\n",
    "            pair_string = f\"{src_exact}:{tgt_exact}:{src_exact}\"\n",
    "            dp_string   = f\"{dp_half[rid_s]}:{dp_join[rid_t]}:{dp_half[rid_s]}\"\n",
    "\n",
    "            # finalize weighted logit-mean probs\n",
    "            p_sigma   = float(_sigmoid(sumL_p_sigma[i] / max(sumW_sigma[i], 1e-12)))\n",
    "            p_epsilon = float(_sigmoid(sumL_p_eps[i]   / max(sumW_eps[i],   1e-12)))\n",
    "            p_lp      = float(_sigmoid(sumL_p_lp[i]    / max(sumW_lp[i],    1e-12)))\n",
    "\n",
    "            row = {\n",
    "                \"pair_string\": pair_string,\n",
    "                \"dp_string\": dp_string,\n",
    "                # \"support_folds\": int(support[i]),\n",
    "                \"p_sigma\":   p_sigma,\n",
    "                \"p_epsilon\": p_epsilon,\n",
    "                \"p_lp\":      p_lp,\n",
    "            }\n",
    "            out_rows.append(row)\n",
    "\n",
    "    df_pred = pd.DataFrame(out_rows)\n",
    "\n",
    "    # ---- Optional LP filtering ----\n",
    "    if LP_FILTER is not None and not df_pred.empty:\n",
    "        df_pred = df_pred[df_pred[\"p_lp\"] > float(LP_FILTER)].reset_index(drop=True)\n",
    "\n",
    "    # ---- Sort (rank) results — LP first, then σ/ε ----\n",
    "    if not df_pred.empty:\n",
    "        df_pred = df_pred.sort_values(\n",
    "            by=[\"p_lp\", \"p_sigma\", \"p_epsilon\"],\n",
    "            ascending=False\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    return df_pred\n",
    "\n",
    "fold_scores = None  # or list of floats\n",
    "\n",
    "df_pred = predict_links_for_homopolymers(\n",
    "    fold_artifacts,\n",
    "    fold_weights=fold_scores,   # <--- pass weights here (None => uniform)\n",
    "    MIN_SUPPORT=0,\n",
    "    LP_FILTER=0.0\n",
    ")\n",
    "\n",
    "out_txt = \"predicted_links.txt\"\n",
    "df_pred.to_csv(out_txt, sep=\"\\t\", index=False)\n",
    "print(f\"Saved {len(df_pred)} rows to {out_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35da2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
